{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Regressão\n",
    "\n",
    "- Moisés Botarro Ferraz Silva, 8504135\n",
    "- Thales de Lima Kobosighawa,  9897884\n",
    "- Victor Rozzatti Tornisiello, 9806867"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação de um MultiLayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o problema de Regressão, iremos utilizar mesma classe MLP criada anteriormente. Entretanto, ao instanciá-la, iremos utilizar o construtor MLPRegressor. A diferença de comportamento com o MLPClassifier ocorre apenas na conversão do vetor de saída da rede neural. Como não trabalhamos com classes na regressão, não há a necessidade de conversão entre os vetores binários que saem da rede e labels de classes. Podemos tomar os próprios valores que saem da rede como os valores preditos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.utils import shuffle as shuffle_data\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# Layer represents a MLP Layer\n",
    "# It has two main properties:\n",
    "#      - a weigth matrix containing the weights of the layer's neurons. Each line represents a neuron and \n",
    "#        the columns represent its corresponding weights\n",
    "#      - a bias vector, containing the neurons's bias\n",
    "# Since during the backpropagation we need to compute the weights variation using the old ones, the \n",
    "# updated_weights and updated_bias properties store the new values until the update method is called\n",
    "class Layer:\n",
    "    # Create a new Layer with 'size' neurons, each one linked to 'inputs_size' inputs\n",
    "    def __init__(self, size, inputs_size):\n",
    "        self.size = size\n",
    "        self.inputs_size = inputs_size\n",
    "        self.weights = np.array([[random.uniform(-0.1, 0.1) for j in range(inputs_size)] for i in range(size)])\n",
    "        self.bias = np.array([random.uniform(-0.1,0.1) for i in range(size)])\n",
    "        \n",
    "        self.d_weights_current = np.zeros((size, inputs_size))\n",
    "        self.d_bias_current = np.zeros(size)\n",
    "        self.d_weights_old = np.zeros((size, inputs_size))\n",
    "        self.d_bias_old = np.zeros(size)\n",
    "    \n",
    "    # update updates the weights and bias matrices with the values stored in the updated ones\n",
    "    def update(self, eta, alpha):\n",
    "        #self.weights = np.copy(self.updated_weights)\n",
    "        #self.bias = np.copy(self.updated_bias)\n",
    "        \n",
    "        self.weights = self.weights + eta*self.d_weights_current + alpha*self.d_weights_old \n",
    "        self.bias = self.bias + eta*self.d_bias_current + alpha*self.d_bias_old\n",
    "        \n",
    "        self.d_weights_old = self.d_weights_current\n",
    "        self.d_bias_old = self.d_bias_current\n",
    "        \n",
    "    # description prints a layer description\n",
    "    def description(self):\n",
    "        print(\"Layer Info\")\n",
    "        print(\"Weights: \\n\", self.weights)\n",
    "        print(\"Bias: \\n \", self.bias)\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0/(1.0+ math.exp(-x))\n",
    "\n",
    "logistic_vec = np.vectorize(logistic)\n",
    "\n",
    "def logistic_derivate(x):\n",
    "    return x*(1.0-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    # MLP creation. One might pass the MLP layers as parameters or add them later using the add_layer method.\n",
    "    # The classification parameter defines if the MLP will be used for a classification or regression problem\n",
    "    def __init__(self, *layers, classifier=True):\n",
    "        self.classifier = classifier\n",
    "        if classifier:\n",
    "            # Map each class label to a vector with a single 1\n",
    "            # Ex: Class 0 -> [1,0]\n",
    "            #     Class 1 -> [0,1]\n",
    "            self.class_mapping = dict()  \n",
    "            # Unmap each class vector to the corresponding class label\n",
    "            # Ex: [1,0] -> Class 0 \n",
    "            #     [0,1] -> Class 1\n",
    "            self.class_unmapping = dict()\n",
    "            \n",
    "        self.layers = list()\n",
    "        for layer in layers:\n",
    "            self.add_layer(layer)\n",
    "    \n",
    "    # Shortcut to create a classifier MLP\n",
    "    @classmethod\n",
    "    def MLPClassifier(cls, *layers):\n",
    "        return cls(classifier=True, *layers)   \n",
    "    \n",
    "    # Shortcut to create a regressor MLP\n",
    "    @classmethod\n",
    "    def MLPRegressor(cls, *layers):\n",
    "        return cls(classifier=False, *layers)\n",
    "    \n",
    "    # add_layer adds a new layer on the MLP. It verifies whether or not the new layer is compatible with the MLP\n",
    "    def add_layer(self, layer):\n",
    "        # If there's already a layer in the MLP, verify if the new layer is compatible\n",
    "        if len(self.layers) > 0:\n",
    "            if layer.inputs_size != self.layers[-1].size:\n",
    "                print(\"The new layer is incompatible with the MLP\")\n",
    "                print(\"Please, use a layer where each neuron has the same amount of inputs as the number\" \\\n",
    "                     \"of neurons in the MLP last layer\")\n",
    "        \n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    # description prints the info about the MLP layers\n",
    "    def description(self):\n",
    "        print(\"MLP Classifier?: \", self.classifier)\n",
    "        print(\"-------------------------\")\n",
    "        print(\"MLP Info:\")\n",
    "        for layer, i in zip(self.layers, range(len(self.layers))):\n",
    "            print(\"--- Layer: %d ---\" % i)\n",
    "            layer.description()\n",
    "            \n",
    "    # __get_class_mapping gets the class labels in the classes list and builds the mapping dicionaries\n",
    "    # class_mapping and class_unmapping\n",
    "    def __get_class_mapping(self, classes):\n",
    "        class_labels = np.unique(classes)\n",
    "        \n",
    "        for c in range(len(class_labels)):\n",
    "            class_label = class_labels[c]\n",
    "            class_vector = np.zeros(len(class_labels))\n",
    "            class_vector[c] = 1\n",
    "    \n",
    "            self.class_mapping[class_label] = class_vector\n",
    "            \n",
    "            # We can't use a list as a hash key. So transform it into a tuple\n",
    "            self.class_unmapping[tuple(class_vector)] = class_label\n",
    "        \n",
    "    # __convert_class_labels_to_vectors converts a list with class labels to a list with \n",
    "    # vectors that maps each class label\n",
    "    def __convert_class_labels_to_vectors(self, class_labels):\n",
    "        return [self.class_mapping[c] for c in class_labels]\n",
    "    \n",
    "    # __convert_class_vectors_to_labels converts a list with class vectors to a list with \n",
    "    # the corresponding class labels\n",
    "    def __convert_class_vectors_to_labels(self, class_vectors):\n",
    "        return [self.class_unmapping[tuple(class_vector)] for class_vector in class_vectors]\n",
    "        \n",
    "        \n",
    "    # fast_forward computes the ouput for a given input vector\n",
    "    def fast_forward(self,input_v):\n",
    "        # We need to store each layer input in order to perform the backpropagation\n",
    "        self.inputs = list()\n",
    "    \n",
    "        # The input is applied in a layer weights matrix and the bias is added in the result\n",
    "        # Then, the logistic function is applied to each layer neuron result\n",
    "        # For a layer, we have a final output vector where each component i represents the output\n",
    "        # of the neuron i\n",
    "        for layer in self.layers:\n",
    "            self.inputs.append(input_v)\n",
    "            output = logistic_vec(layer.weights @ input_v + layer.bias)\n",
    "            \n",
    "            # The output of the current layer is the input of the next one\n",
    "            input_v = output\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    # train trains the MLP using the examples passed in the samples parameter\n",
    "    # The expected output for each example must be passed in the classes parameter;\n",
    "    # eta represents the MLP learning rate;\n",
    "    # tol represents the error tolerance. The MLP is trained until the cumulative squared error for all example\n",
    "    #     is less than the tol value\n",
    "    # print_status prints the output for each example during the training phase\n",
    "    def train(self, samples, classes, eta=0.5, alpha=0, tol=1e-2, epoch_max=2000, \n",
    "              print_status=False, shuffle=True):\n",
    "        # Map the class labels to output vectors if it's a classification problem\n",
    "        if self.classifier:\n",
    "            self.__get_class_mapping(classes)\n",
    "            classes = self.__convert_class_labels_to_vectors(classes)\n",
    "                \n",
    "        error = tol\n",
    "        new_error = 3*tol\n",
    "        epoch = 0\n",
    "        \n",
    "        # The training stops when the max number of epochs is reached or the Kramer and Sangiovanni-Vicentelly\n",
    "        # criteria is valid. According to it, we can consider that the BP converged when the average mean squared\n",
    "        # error is less than a given tolerance\n",
    "        while (abs(new_error - error) > tol and epoch < epoch_max):\n",
    "            epoch += 1\n",
    "            error = 0\n",
    "            new_error = 0\n",
    "            \n",
    "            # Suffles samples to avoid saturation if training with samples beloging to the same class\n",
    "            # one after another\n",
    "            if shuffle:\n",
    "                samples, classes = shuffle_data(samples, classes)\n",
    "            \n",
    "            for input_v, t in zip(samples, classes):  \n",
    "                # ---- Compute the output for the given input vector ----\n",
    "                output = self.fast_forward(input_v)\n",
    "                \n",
    "                # Compute the mean squared error before the backpropagation\n",
    "                error_sample = pow((np.array(t)-np.array(output)),2)\n",
    "                # We need to sum the error of each component when the output is a vector\n",
    "                error += sum(error_sample)/len(samples)\n",
    "                \n",
    "                if (print_status == True):\n",
    "                    print(\"\\ttraining example: %s from class %s\" % (input_v, t), end = \" \")\n",
    "                    print(\"y = \", output)\n",
    "     \n",
    "                # ---- Backpropagation ----\n",
    "                # Compute the new weights of each layer\n",
    "                # Remark: the udpated weights are stored as a layer property and the layer is updated once \n",
    "                # the backpropagation is finished\n",
    "                # It's necessary to do so in order to compute the delta value for the inner layers. We need \n",
    "                # to use the weights that caused the error to compute the delta instead of the updated weights\n",
    "                for l in reversed(range(len(self.layers))): # Traverse the layers in reversed order\n",
    "                    layer = self.layers[l]\n",
    "             \n",
    "                    deltas = list()\n",
    "                    # Compute the delta for each layer neuron n\n",
    "                    for n in range(len(layer.weights)):\n",
    "                        # Last Layer\n",
    "                        if l == (len(self.layers)-1):\n",
    "                            delta = (t[n]-output[n])*logistic_derivate(output[n])\n",
    "                            \n",
    "                        # Inner Layer\n",
    "                        else:\n",
    "                            # output of the current layer is the input of the next one\n",
    "                            neuron_output = self.inputs[l+1][n]\n",
    "                            # weights of each neuron output\n",
    "                            errors_weights = self.layers[l+1].weights[:,n]\n",
    "                            \n",
    "                            delta = np.dot(delta_next_layer,errors_weights)*logistic_derivate(neuron_output)\n",
    "                              \n",
    "                        # Computes the weights and bias variation for the neuron n\n",
    "                        for w in range(len(layer.weights[n])):\n",
    "                            layer.d_weights_current[n][w] = delta*self.inputs[l][w]\n",
    "                        layer.d_bias_current[n] = delta*1 # bias input = 1\n",
    "                        \n",
    "                        #for w in range(len(layer.weights[n])):\n",
    "                        #    layer.updated_weights[n][w] = layer.weights[n][w] + eta*delta*self.inputs[l][w]\n",
    "                        #layer.updated_bias[n] = layer.bias[n] + (eta*delta*1) # bias input = 1\n",
    "\n",
    "                        # Store the neuron delta\n",
    "                        deltas.append(delta)\n",
    "                    \n",
    "                    # The neurons' delta of the current layer will be used to compute the deltas of the \n",
    "                    # next inner layer\n",
    "                    delta_next_layer = np.array(deltas)\n",
    "                     \n",
    "                # Once the backpropagation is finished for the current example, update all the weigths and bias\n",
    "                for layer in self.layers:\n",
    "                    layer.update(eta, alpha)\n",
    "                    \n",
    "                # Compute the new error mean squared error\n",
    "                output = self.fast_forward(input_v)\n",
    "                error_sample = pow((np.array(t)-np.array(output)),2)\n",
    "                #print(\"error sample: \",error_sample)\n",
    "                new_error += sum(error_sample)/len(samples)\n",
    "            \n",
    "            # End of a epoch\n",
    "            if epoch%1 == 0: # Print status only after each 100 iterations \n",
    "                clear_output(wait=True)\n",
    "                display(\"End of epoch \" + str(epoch) + \". Total Error = \" + str(new_error))\n",
    "        \n",
    "        # End of training         \n",
    "        clear_output(wait=True)\n",
    "        display(\"End of epoch \" + str(epoch) + \". Total Error = \" + str(new_error))\n",
    "        \n",
    "    # predicts gets a list of input samples and returns a list with the predicted outputs\n",
    "    def predict(self, samples):\n",
    "        outputs = list()\n",
    "        for input_v in samples:\n",
    "            probs = self.fast_forward(input_v)\n",
    "            \n",
    "            if self.classifier:\n",
    "                class_pos = np.argmax(probs)\n",
    "                output = np.zeros(len(probs))\n",
    "                output[class_pos] = 1\n",
    "            \n",
    "                #outputs.append(self.class_unmapping[tuple(output)])\n",
    "                outputs.append(output)\n",
    "                \n",
    "            else:\n",
    "                outputs.append(probs)\n",
    "    \n",
    "        if self.classifier:\n",
    "            return self.__convert_class_vectors_to_labels(outputs)\n",
    "        \n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré Processamento dos Dados\n",
    "\n",
    "Assim como feito na classificação, iremos pré tatar os dados de forma que todos os atributos e valores de saída de cada amostra da base de dados esteja no intervalo [0,1]. Dessa forma, evitamos a saturação dos neurônios da rede. Além disso, como a função de ativação utilizada é a sigmoide, os seus valores possíveis de saída estão entre 0 e 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data transforms data in order to all points have mean 0 and variance 1\n",
    "def normalize_data(data):\n",
    "    normalized_columns = list()\n",
    "    for c in range(len(data[0])):\n",
    "        col = data[:,c]\n",
    "        normalized_columns.append((col - np.mean(col))/np.std(col))\n",
    "\n",
    "    return np.array(normalized_columns).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_data transforms data in order to all points be in the interval [0,1]\n",
    "def scale_data(data):\n",
    "    normalized_columns = list()\n",
    "    for c in range(len(data[0])):\n",
    "        col = data[:,c]\n",
    "        normalized_columns.append((col-np.min(col))/(np.max(col)-np.min(col)))\n",
    "\n",
    "    return np.array(normalized_columns).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para avaliação do modelo de regressão, iremos calcular o erro quadrático médio existente entre os valores preditos e os valores esperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate(real_outputs, predicted_outputs):\n",
    "    acc = mean_squared_error(real_outputs, predicted_outputs)\n",
    "    print(\"MSE: %.4f\" % (acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudos dos Meta Parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura do Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ler nosso conjunto de dados e escalá-los, inclusive as saídas esperadas para que fiquem no entre 0 e 1, evitando a saturação dos neurônios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('default_features_1059_tracks.txt', header=None)\n",
    "df.head(5)\n",
    "\n",
    "data = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale input and outpt to be in the range(0,1)\n",
    "scaled = scale_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conjunto de dados consiste em 68 features com as duas últimas colunas referindo-se à origem de cada música, representando sua longitude e latitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = scaled[:,:-2]\n",
    "outputs = scaled[:,-2:]\n",
    "n_outputs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novamente, vamos realizar as análises referentes à arquitetura da rede e parâmetros de aprendizado dividindo o conjunto de dados em 70% de treinamento e 30% de teste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão em Conjunto de Treinamento e Teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste de diferentes Arquiteturas de Camadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim com feito para a classificação, vamos considerar diferentes arquiteturas de rede, com uma e duas camadas. Entretanto, ao variar o número de neurônios, não o igualaremos ao número de features, à sua metade e ao seu dobro, uma vez que o treinamento para uma camada com apenas 10 neurônios já é lento. Consideraremos camadas com 1/4, 1/2 e igual ao número de features.\n",
    "\n",
    "Tomaremos apenas 100 ciclos como epoch durante essa etapa!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "accs = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Layer - 17 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 17\n",
    "N = len(X_train[0])\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPRegressor(Layer(N1, N), Layer(n_outputs, N1))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "accs[(N1, 0)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Layer -  34 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 34\n",
    "N = len(X_train[0])\n",
    "n_outputs = len(y_train[0])\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPRegressor(Layer(N1, N), Layer(n_outputs, N1))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "accs[(N1, 0)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Layer - 68 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 68\n",
    "N = len(X_train[0])\n",
    "n_outputs = len(y_train[0])\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPRegressor(Layer(N1, N), Layer(n_outputs, N1))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "accs[(N1, 0)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 17 Neurons - 17 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 17\n",
    "N2 = 17\n",
    "N = len(X_train[0])\n",
    "n_outputs = len(y_train[0])\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPRegressor(Layer(N1, N), Layer(N2, N1),Layer(n_outputs, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "accs[(N1, N2)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 17 Neurons - 34 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 17\n",
    "N2 = 34\n",
    "N = len(X_train[0])\n",
    "n_outputs = len(y_train[0])\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPRegressor(Layer(N1, N), Layer(N2, N1),Layer(n_outputs, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "accs[(N1, N2)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 17 Neurons - 68 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 17\n",
    "N2 = 68\n",
    "N = len(X_train[0])\n",
    "n_outputs = len(y_train[0])\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPRegressor(Layer(N1, N), Layer(N2, N1),Layer(n_outputs, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "accs[(N1, N2)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 34 Neurons - 17 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 34\n",
    "N2 = 17\n",
    "N = len(X_train[0])\n",
    "n_outputs = len(y_train[0])\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPRegressor(Layer(N1, N), Layer(N2, N1),Layer(n_outputs, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "accs[(N1, N2)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 34 Neurons - 34 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 34\n",
    "N2 = 34\n",
    "N = len(X_train[0])\n",
    "n_outputs = len(y_train[0])\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPRegressor(Layer(N1, N), Layer(N2, N1),Layer(n_outputs, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "accs[(N1, N2)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 34 Neurons - 68 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 34\n",
    "N2 = 68\n",
    "N = len(X_train[0])\n",
    "n_outputs = len(y_train[0])\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPRegressor(Layer(N1, N), Layer(N2, N1),Layer(n_outputs, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "accs[(N1, N2)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 68 Neurons - 17 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 68\n",
    "N2 = 17\n",
    "N = len(X_train[0])\n",
    "n_outputs = len(y_train[0])\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPRegressor(Layer(N1, N), Layer(N2, N1),Layer(n_outputs, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "accs[(N1, N2)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 68 Neurons - 34 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 68\n",
    "N2 = 34\n",
    "N = len(X_train[0])\n",
    "n_outputs = len(y_train[0])\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPRegressor(Layer(N1, N), Layer(N2, N1),Layer(n_outputs, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "accs[(N1, N2)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 68 Neurons - 34 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 68\n",
    "N2 = 68\n",
    "N = len(X_train[0])\n",
    "n_outputs = len(y_train[0])\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPRegressor(Layer(N1, N), Layer(N2, N1),Layer(n_outputs, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "accs[(N1, N2)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 1\n",
    "best_N1 = 0\n",
    "best_N2 = 0\n",
    "for layers, acc in accs.items():\n",
    "    if acc < best_acc:\n",
    "        best_acc = acc\n",
    "        best_N1 = layers[0]\n",
    "        best_N2 = layers[1]\n",
    "\n",
    "print(\"best MSE: \", best_acc)\n",
    "print(\"best N1: \", best_N1)\n",
    "print(\"best N2: \", best_N2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Número de epochs usadas durante o Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_train = mlp.predict(X_train)\n",
    "predicted_test = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TRAINING SET ===\")\n",
    "evaluate(y_train, predicted_train)\n",
    "print(\"=== TEST SET ===\")\n",
    "accs[epochs] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_train = mlp.predict(X_train)\n",
    "predicted_test = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TRAINING SET ===\")\n",
    "evaluate(y_train, predicted_train)\n",
    "print(\"=== TEST SET ===\")\n",
    "accs[epochs] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 400 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 400\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_train = mlp.predict(X_train)\n",
    "predicted_test = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TRAINING SET ===\")\n",
    "evaluate(y_train, predicted_train)\n",
    "print(\"=== TEST SET ===\")\n",
    "accs[epochs] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 800 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 800\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_train = mlp.predict(X_train)\n",
    "predicted_test = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TRAINING SET ===\")\n",
    "evaluate(y_train, predicted_train)\n",
    "print(\"=== TEST SET ===\")\n",
    "accs[epochs] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_train = mlp.predict(X_train)\n",
    "predicted_test = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TRAINING SET ===\")\n",
    "evaluate(y_train, predicted_train)\n",
    "print(\"=== TEST SET ===\")\n",
    "accs[epochs] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = 0\n",
    "best_mse = 1\n",
    "\n",
    "for epoch, mse in accs.items():\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_epoch = epoch\n",
    "\n",
    "print(\"Best MSE:\", best_mse)\n",
    "print(\"Best epoch:\", best_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adicionar comentário sobre possível overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate and Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.3 e Momentum = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.3\n",
    "alpha = 0.3\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epoch, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_test = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.3 e Momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.3\n",
    "alpha = 0.5\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epoch, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_test = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.3 e Momentum = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.3\n",
    "alpha = 0.8\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epoch, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_test = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.5 e Momentum = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.5\n",
    "alpha = 0.3\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epoch, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_test = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.5 e Momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.5\n",
    "alpha = 0.5\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epoch, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_test = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.5 e Momentum = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.5\n",
    "alpha = 0.8\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epoch, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_test = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.8 e Momentum = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.8\n",
    "alpha = 0.3\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epoch, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_test = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.8 e Momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.8\n",
    "alpha = 0.5\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epoch, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_test = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.8 e Momentum = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.8\n",
    "alpha = 0.8\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epoch, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_test = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 1 e Momentum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1\n",
    "alpha = 1\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epoch, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_test = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mse = pd.DataFrame()\n",
    "\n",
    "best_eta = 0\n",
    "best_alpha = 0\n",
    "best_mse = 1\n",
    "for run, mse in accs.items():\n",
    "    total_mse.at[str(run[0]), str(run[1])] = mse\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_eta = run[0]\n",
    "        best_alpha = run[1]\n",
    "    \n",
    "print(total_mse)\n",
    "\n",
    "print(\"Best MSE: \", best_mse)\n",
    "print(\"Best eta: \", best_eta)\n",
    "print(\"Best alpha: \", best_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variação do tamanho dos conjuntos de treinamento e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos variar o tamanho dos conjuntos de treinamento e teste utilizando a melhor arquitetura encontrada acima e os melhores valores de learning rate e momentum. Utilizaremos incialmente 70% dos dados para treinamento, aumentando gradativamente esse valor até 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 70% for training, 30% for test¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=test_size,random_state=42)\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epoch, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted_test = mlp.predict(X_test)\n",
    "accs[test_size] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 75% for training, 25% for test¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.25\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=test_size,random_state=42)\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epoch, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted_test = mlp.predict(X_test)\n",
    "accs[test_size] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80% for training, 20% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=test_size,random_state=42)\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epoch, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted_test = mlp.predict(X_test)\n",
    "accs[test_size] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 85% for training, 15% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.15\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=test_size,random_state=42)\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epoch, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted_test = mlp.predict(X_test)\n",
    "accs[test_size] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 90% for training, 10% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=test_size,random_state=42)\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epoch, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted_test = mlp.predict(X_test)\n",
    "accs[test_size] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 95% for training, 5% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.05\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=test_size,random_state=42)\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), ]\n",
    "if best_N2 != 0:\n",
    "    best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "else:\n",
    "    best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "mlp = MLP.MLPRegressor(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epoch, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted_test = mlp.predict(X_test)\n",
    "accs[test_size] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_size = 0\n",
    "best_mse = 1\n",
    "\n",
    "for test_size, mse in accs.items():\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_test_size = test_size\n",
    "        \n",
    "print(\"Best MSE: \", best_mse)\n",
    "print(\"Best test size: \", best_test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação do Modelo final com Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "mses_train = list()\n",
    "mses_test = list()\n",
    "\n",
    "for train_index, test_index in kf.split(inputs, outputs):\n",
    "    X_train, X_test = inputs[train_index], inputs[test_index]\n",
    "    y_train, y_test = outputs[train_index], outputs[test_index]\n",
    "    \n",
    "    random.seed(0)\n",
    "    best_layers = [Layer(best_N1, N), ]\n",
    "    if best_N2 != 0:\n",
    "        best_layers.extend([Layer(best_N2, best_N1), Layer(n_outputs, best_N2)])\n",
    "    else:\n",
    "        best_layers.extend([Layer(n_outputs, best_N1)])\n",
    "    \n",
    "    mlp = MLP.MLPRegressor(*best_layers)\n",
    "    mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epoch, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "    predicted_train  = mlp.predict(X_train)\n",
    "    predicted_test  = mlp.predict(X_test)\n",
    "    \n",
    "    mses_train.append(evaluate(y_train, predicted_train))\n",
    "    mses_test.append(evaluate(y_test, predicted_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==== MSE in the TRAINING SETS ====\")\n",
    "for (fold, mse_train) in zip(range(10), mses_train):\n",
    "    print(\"Fold: %d\\tMSE: %.4f\" % (fold, mse_train))\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"MSE Médio:   %.4f\" % np.mean(mses_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==== MSE in the TEST SETS ====\")\n",
    "for (fold, mse_test) in zip(range(10), mses_test):\n",
    "    print(\"Fold: %d\\tMSE: %.4f\" % (fold, mse_test))\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"MSE Médio:   %.4f\" % np.mean(mses_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
