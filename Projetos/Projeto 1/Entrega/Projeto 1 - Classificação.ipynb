{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Moisés Botarro Ferraz Silva, 8504135\n",
    "- Thales de Lima Kobosighawa,  9897884\n",
    "- Victor Rozzatti Tornisiello, 9806867"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação de um MultiLayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar o Multilayer Perceptron, pegamos as classes Layer e MLP implementadas para o Laboratório 2. As seguintes alterações foram realizadas:\n",
    "\n",
    "### Classe Layer\n",
    "- Adição dos atributos *d_weights_current*, *d_weights_old*, *d_bias_curent* e *d_bias_old* na classe Layer para aplicar a regra delta com parâmetro momentum ao realizar a atualização dos pesos da rede \n",
    "\n",
    "### Classe MLP\n",
    "- Criação dos construtores MLPClassifier e MLPRegressor para a classe MLP, a fim de inicializar a classe para realizar uma classsificação ou regressão, respectivamente\n",
    "- Adição dos métodos *\\__get_class_mapping*, *\\__convert_class_labels_to_vectors* e *\\__convert_class_vectors_to_labels* na classe MLP para fazer a conversão de labels de classe em vetores binários. Por exemplo, podemos passar para o MLP um conjunto de treinamento com as classes (classeA, classeB, classeC). Esses métodos transformarão essas classes nos vetores (100, 010 e 001), evitando que a conversão seja feita pelo usuário a todo momento. Vale relembrar que esse mapeamento é realizado apenas para o caso de classificação\n",
    "- Dentro de cada epoch durante o treinamento, os exemplos são embaralhados com o auxílio da função *shuffle* para evitar a saturação da saída dos neurônios\n",
    "- Adição do critério de Kramer e Sangiovanni-Vicentelli para determinar a convergência do treinamento. Para-se de treinar quando a norma do gradiente dos pesos é menor que uma tolerância desejada\n",
    "- Possibilidade de passar número máximo de epochs para treinamento para interrompé-lo antes que o gradiente dos pesos seja menor que a tolerância desejada. Isso será útil quando o conjunto de treinamento é muito grande e não consegue-se classificar corretamente todos os exemplos.\n",
    "- Mapeamento dos vetores binários da saída de rede para labels de classe no método *predict* durante a classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.utils import shuffle as shuffle_data\n",
    "\n",
    "# Layer represents a MLP Layer\n",
    "# It has two main properties:\n",
    "#      - a weigth matrix containing the weights of the layer's neurons. Each line represents a neuron and \n",
    "#        the columns represent its corresponding weights\n",
    "#      - a bias vector, containing the neurons's bias\n",
    "# Since during the backpropagation we need to compute the weights variation using the old ones, the \n",
    "# updated_weights and updated_bias properties store the new values until the update method is called\n",
    "class Layer:\n",
    "    # Create a new Layer with 'size' neurons, each one linked to 'inputs_size' inputs\n",
    "    def __init__(self, size, inputs_size):\n",
    "        self.size = size\n",
    "        self.inputs_size = inputs_size\n",
    "        self.weights = np.array([[random.uniform(-0.1, 0.1) for j in range(inputs_size)] for i in range(size)])\n",
    "        self.bias = np.array([random.uniform(-0.1,0.1) for i in range(size)])\n",
    "        \n",
    "        self.d_weights_current = np.zeros((size, inputs_size))\n",
    "        self.d_bias_current = np.zeros(size)\n",
    "        self.d_weights_old = np.zeros((size, inputs_size))\n",
    "        self.d_bias_old = np.zeros(size)\n",
    "    \n",
    "    # update updates the weights and bias matrices with the values stored in the updated ones\n",
    "    def update(self, eta, alpha):        \n",
    "        self.weights = self.weights + eta*self.d_weights_current + alpha*self.d_weights_old \n",
    "        self.bias = self.bias + eta*self.d_bias_current + alpha*self.d_bias_old\n",
    "        \n",
    "        self.d_weights_old = self.d_weights_current\n",
    "        self.d_bias_old = self.d_bias_current\n",
    "        \n",
    "    # description prints a layer description\n",
    "    def description(self):\n",
    "        print(\"Layer Info\")\n",
    "        print(\"Weights: \\n\", self.weights)\n",
    "        print(\"Bias: \\n \", self.bias)\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0/(1.0+ math.exp(-x))\n",
    "\n",
    "logistic_vec = np.vectorize(logistic)\n",
    "\n",
    "def logistic_derivate(x):\n",
    "    return x*(1.0-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    # MLP creation. One might pass the MLP layers as parameters or add them later using the add_layer method.\n",
    "    # The classification parameter defines if the MLP will be used for a classification or regression problem\n",
    "    def __init__(self, *layers, classifier=True):\n",
    "        self.classifier = classifier\n",
    "        if classifier:\n",
    "            # Map each class label to a vector with a single 1\n",
    "            # Ex: Class 0 -> [1,0]\n",
    "            #     Class 1 -> [0,1]\n",
    "            self.class_mapping = dict()  \n",
    "            # Unmap each class vector to the corresponding class label\n",
    "            # Ex: [1,0] -> Class 0 \n",
    "            #     [0,1] -> Class 1\n",
    "            self.class_unmapping = dict()\n",
    "            \n",
    "        self.layers = list()\n",
    "        for layer in layers:\n",
    "            self.add_layer(layer)\n",
    "    \n",
    "    # Shortcut to create a classifier MLP\n",
    "    @classmethod\n",
    "    def MLPClassifier(cls, *layers):\n",
    "        return cls(classifier=True, *layers)   \n",
    "    \n",
    "    # Shortcut to create a regressor MLP\n",
    "    @classmethod\n",
    "    def MLPRegressor(cls, *layers):\n",
    "        return cls(classifier=False, *layers)\n",
    "    \n",
    "    # add_layer adds a new layer on the MLP. It verifies whether or not the new layer is compatible with the MLP\n",
    "    def add_layer(self, layer):\n",
    "        # If there's already a layer in the MLP, verify if the new layer is compatible\n",
    "        if len(self.layers) > 0:\n",
    "            if layer.inputs_size != self.layers[-1].size:\n",
    "                print(\"The new layer is incompatible with the MLP\")\n",
    "                print(\"Please, use a layer where each neuron has the same amount of inputs as the number\" \\\n",
    "                     \"of neurons in the MLP last layer\")\n",
    "        \n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    # description prints the info about the MLP layers\n",
    "    def description(self):\n",
    "        print(\"MLP Classifier?: \", self.classifier)\n",
    "        print(\"-------------------------\")\n",
    "        print(\"MLP Info:\")\n",
    "        for layer, i in zip(self.layers, range(len(self.layers))):\n",
    "            print(\"--- Layer: %d ---\" % i)\n",
    "            layer.description()\n",
    "            \n",
    "    # __get_class_mapping gets the class labels in the classes list and builds the mapping dicionaries\n",
    "    # class_mapping and class_unmapping\n",
    "    def __get_class_mapping(self, classes):\n",
    "        class_labels = np.unique(classes)\n",
    "        \n",
    "        for c in range(len(class_labels)):\n",
    "            class_label = class_labels[c]\n",
    "            class_vector = np.zeros(len(class_labels))\n",
    "            class_vector[c] = 1\n",
    "    \n",
    "            self.class_mapping[class_label] = class_vector\n",
    "            \n",
    "            # We can't use a list as a hash key. So transform it into a tuple\n",
    "            self.class_unmapping[tuple(class_vector)] = class_label\n",
    "        \n",
    "    # __convert_class_labels_to_vectors converts a list with class labels to a list with \n",
    "    # vectors that maps each class label\n",
    "    def __convert_class_labels_to_vectors(self, class_labels):\n",
    "        return [self.class_mapping[c] for c in class_labels]\n",
    "    \n",
    "    # __convert_class_vectors_to_labels converts a list with class vectors to a list with \n",
    "    # the corresponding class labels\n",
    "    def __convert_class_vectors_to_labels(self, class_vectors):\n",
    "        return [self.class_unmapping[tuple(class_vector)] for class_vector in class_vectors]\n",
    "        \n",
    "        \n",
    "    # fast_forward computes the ouput for a given input vector\n",
    "    def fast_forward(self,input_v):\n",
    "        # We need to store each layer input in order to perform the backpropagation\n",
    "        self.inputs = list()\n",
    "    \n",
    "        # The input is applied in a layer weights matrix and the bias is added in the result\n",
    "        # Then, the logistic function is applied to each layer neuron result\n",
    "        # For a layer, we have a final output vector where each component i represents the output\n",
    "        # of the neuron i\n",
    "        for layer in self.layers:\n",
    "            self.inputs.append(input_v)\n",
    "            output = logistic_vec(layer.weights @ input_v + layer.bias)\n",
    "            \n",
    "            # The output of the current layer is the input of the next one\n",
    "            input_v = output\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    # train trains the MLP using the examples passed in the samples parameter\n",
    "    # The expected output for each example must be passed in the classes parameter;\n",
    "    # eta represents the MLP learning rate;\n",
    "    # tol represents the error tolerance. The MLP is trained until the cumulative squared error for all example\n",
    "    #     is less than the tol value\n",
    "    # print_status prints the output for each example during the training phase\n",
    "    def train(self, samples, classes, eta=0.5, alpha=0, tol=1e-2, epoch_max=2000, \n",
    "              print_status=False, shuffle=True):\n",
    "        # Map the class labels to output vectors if it's a classification problem\n",
    "        if self.classifier:\n",
    "            self.__get_class_mapping(classes)\n",
    "            classes = self.__convert_class_labels_to_vectors(classes)\n",
    "                \n",
    "        error = tol\n",
    "        new_error = 3*tol\n",
    "        epoch = 0\n",
    "        \n",
    "        # The training stops when the max number of epochs is reached or the Kramer and Sangiovanni-Vicentelly\n",
    "        # criteria is valid. According to it, we can consider that the BP converged when the average mean squared\n",
    "        # error is less than a given tolerance\n",
    "        while (abs(new_error - error) > tol and epoch < epoch_max):\n",
    "            epoch += 1\n",
    "            error = 0\n",
    "            new_error = 0\n",
    "            \n",
    "            # Suffles samples to avoid saturation if training with samples beloging to the same class\n",
    "            # one after another\n",
    "            if shuffle:\n",
    "                samples, classes = shuffle_data(samples, classes)\n",
    "            \n",
    "            for input_v, t in zip(samples, classes):  \n",
    "                # ---- Compute the output for the given input vector ----\n",
    "                output = self.fast_forward(input_v)\n",
    "                \n",
    "                # Compute the mean squared error before the backpropagation\n",
    "                error_sample = pow((np.array(t)-np.array(output)),2)\n",
    "                # We need to sum the error of each component when the output is a vector\n",
    "                error += sum(error_sample)/len(samples)\n",
    "                \n",
    "                if (print_status == True):\n",
    "                    print(\"\\ttraining example: %s from class %s\" % (input_v, t), end = \" \")\n",
    "                    print(\"y = \", output)\n",
    "     \n",
    "                # ---- Backpropagation ----\n",
    "                # Compute the new weights of each layer\n",
    "                # Remark: the udpated weights are stored as a layer property and the layer is updated once \n",
    "                # the backpropagation is finished\n",
    "                # It's necessary to do so in order to compute the delta value for the inner layers. We need \n",
    "                # to use the weights that caused the error to compute the delta instead of the updated weights\n",
    "                for l in reversed(range(len(self.layers))): # Traverse the layers in reversed order\n",
    "                    layer = self.layers[l]\n",
    "             \n",
    "                    deltas = list()\n",
    "                    # Compute the delta for each layer neuron n\n",
    "                    for n in range(len(layer.weights)):\n",
    "                        # Last Layer\n",
    "                        if l == (len(self.layers)-1):\n",
    "                            delta = (t[n]-output[n])*logistic_derivate(output[n])\n",
    "                            \n",
    "                        # Inner Layer\n",
    "                        else:\n",
    "                            # output of the current layer is the input of the next one\n",
    "                            neuron_output = self.inputs[l+1][n]\n",
    "                            # weights of each neuron output\n",
    "                            errors_weights = self.layers[l+1].weights[:,n]\n",
    "                            \n",
    "                            delta = np.dot(delta_next_layer,errors_weights)*logistic_derivate(neuron_output)\n",
    "                              \n",
    "                        # Computes the weights and bias variation for the neuron n\n",
    "                        for w in range(len(layer.weights[n])):\n",
    "                            layer.d_weights_current[n][w] = delta*self.inputs[l][w]\n",
    "                        layer.d_bias_current[n] = delta*1 # bias input = 1\n",
    "                        \n",
    "                        #for w in range(len(layer.weights[n])):\n",
    "                        #    layer.updated_weights[n][w] = layer.weights[n][w] + eta*delta*self.inputs[l][w]\n",
    "                        #layer.updated_bias[n] = layer.bias[n] + (eta*delta*1) # bias input = 1\n",
    "\n",
    "                        # Store the neuron delta\n",
    "                        deltas.append(delta)\n",
    "                    \n",
    "                    # The neurons' delta of the current layer will be used to compute the deltas of the \n",
    "                    # next inner layer\n",
    "                    delta_next_layer = np.array(deltas)\n",
    "                     \n",
    "                # Once the backpropagation is finished for the current example, update all the weigths and bias\n",
    "                for layer in self.layers:\n",
    "                    layer.update(eta, alpha)\n",
    "                    \n",
    "                # Compute the new error mean squared error\n",
    "                output = self.fast_forward(input_v)\n",
    "                error_sample = pow((np.array(t)-np.array(output)),2)\n",
    "                #print(\"error sample: \",error_sample)\n",
    "                new_error += sum(error_sample)/len(samples)\n",
    "            \n",
    "            # End of a epoch\n",
    "            if epoch%1 == 0: # Print status only after each 100 iterations \n",
    "                clear_output(wait=True)\n",
    "                display(\"End of epoch \" + str(epoch) + \". Total Error = \" + str(new_error))\n",
    "        \n",
    "        # End of training         \n",
    "        clear_output(wait=True)\n",
    "        display(\"End of epoch \" + str(epoch) + \". Total Error = \" + str(new_error))\n",
    "        \n",
    "    # predicts gets a list of input samples and returns a list with the predicted outputs\n",
    "    def predict(self, samples):\n",
    "        outputs = list()\n",
    "        for input_v in samples:\n",
    "            probs = self.fast_forward(input_v)\n",
    "            \n",
    "            if self.classifier:\n",
    "                class_pos = np.argmax(probs)\n",
    "                output = np.zeros(len(probs))\n",
    "                output[class_pos] = 1\n",
    "            \n",
    "                #outputs.append(self.class_unmapping[tuple(output)])\n",
    "                outputs.append(output)\n",
    "                \n",
    "            else:\n",
    "                outputs.append(probs)\n",
    "    \n",
    "        if self.classifier:\n",
    "            return self.__convert_class_vectors_to_labels(outputs)\n",
    "        \n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré Processamento dos Dados\n",
    "\n",
    "A fim de normalizar os dados e evitar a saturação dos neurônios, vamos escalar os dados utilizando a seguinte transformação:\n",
    "\n",
    "$$ x = \\frac{x-x_{min}}{x_{max}-x_{min}}$$\n",
    "\n",
    "Onde $x_{max}$ e $x_{min}$ são, respectivamente, os maiores e menores valores para um determinado atributo.\n",
    "\n",
    "Dessa forma, garantimos que todos os dados inseridos na rede estejam no intervalo [0,1].\n",
    "\n",
    "A função normalize_data transforma os dados de forma que eles tenham média 0 e variância 1. Entretanto, como isso não garante que os valores estejam no intervalo [0,1], não iremos utilizá-la. Realizamos alguns testes e em algusn dos casos, ocorre a saturação dos neurônios utilizando essa transformação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data transforms data in order to all points have mean 0 and variance 1\n",
    "def normalize_data(data):\n",
    "    normalized_columns = list()\n",
    "    for c in range(len(data[0])):\n",
    "        col = data[:,c]\n",
    "        normalized_columns.append((col - np.mean(col))/np.std(col))\n",
    "\n",
    "    return np.array(normalized_columns).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_data transforms data in order to all points be in the interval [0,1]\n",
    "def scale_data(data):\n",
    "    normalized_columns = list()\n",
    "    for c in range(len(data[0])):\n",
    "        col = data[:,c]\n",
    "        normalized_columns.append((col-np.min(col))/(np.max(col)-np.min(col)))\n",
    "\n",
    "    return np.array(normalized_columns).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fim de avaliar o resultado do modelo, vamos calcular a sua acurácia atravéz da função *evaluate* abaixo. Ela calcula a acurância geral comparando um vetor com as classes reais e as classes preditas utilizando o modelo. Além disso, para avaliar a acurácia para cada uma das classes separadamente, a matriz de confusão é construída e através dela, determina-se o desempenho do modelo para cada classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate(real_classes, predicted_classes, display=True):\n",
    "    acc = accuracy_score(real_classes, predicted_classes)\n",
    "    \n",
    "    class_labels = np.sort(np.unique(real_classes))\n",
    "    cm = confusion_matrix(real_classes, predicted_classes, labels=class_labels)\n",
    "    df = pd.DataFrame(cm)\n",
    "    df.columns = class_labels\n",
    "    df.index = class_labels\n",
    "    \n",
    "    # Acurracy per class\n",
    "    accs = list()\n",
    "    for c in range(len(cm)):\n",
    "        accs.append(cm[c,c]/sum(cm[c,:]))\n",
    "    df[\"Accuracy\"] = accs\n",
    "    \n",
    "    avg_acc = np.average(accs)\n",
    "    \n",
    "    if display == True:\n",
    "        print(\"Accuracy: %.2f%%\" % (acc*100))\n",
    "        \n",
    "        print(\"Confusion Matrix and Accuracy per class:\")\n",
    "        print(df)\n",
    "\n",
    "        print(\"Average accuracy per class: %.2f%%\" % (avg_acc*100))\n",
    "        \n",
    "    return acc, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudos dos Meta Parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o problema de classificação, vamos pegar o conjunto de dados fornecidos e vamos normalizá-los de forma que cada valor de feature esteja no intervalo [0,1]. Isso será feito para evitar a saturação da saída dos neurônios e melhorar a convergência do algoritmo de aprendizagem.\n",
    "\n",
    "A seguir, tomaremos um conjunto de treinamento consistindo em 70% da base original. Iremos avaliar o impacto da arquitetura da rede assim como da variação de parâmetros de aprendizado no valor de acurácia obtido na classificação do conjunto de teste. \n",
    "\n",
    "Iremos calcular a acurácia total e a acurácia por classe. Uma vez que não estamos impondo penalidades diferentes para erros cometidos em determinadas classes, vamos considerar como melhor arquitetura aquela que fornece maior valor para a acurácia total, embora isso possa não refletir em uma acurácia por classe elevada! Mais tarde, iremos tratar melhor o balanceamento entre as classes a fim de que o aprendizado da rede seja adequado para todas elas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "0           0            7.4              0.70         0.00             1.9   \n",
       "1           1            7.8              0.88         0.00             2.6   \n",
       "2           2            7.8              0.76         0.04             2.3   \n",
       "3           3           11.2              0.28         0.56             1.9   \n",
       "4           4            7.4              0.70         0.00             1.9   \n",
       "\n",
       "   chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
       "0      0.076                 11.0                  34.0   0.9978  3.51   \n",
       "1      0.098                 25.0                  67.0   0.9968  3.20   \n",
       "2      0.092                 15.0                  54.0   0.9970  3.26   \n",
       "3      0.075                 17.0                  60.0   0.9980  3.16   \n",
       "4      0.076                 11.0                  34.0   0.9978  3.51   \n",
       "\n",
       "   sulphates  alcohol category  \n",
       "0       0.56      9.4      Mid  \n",
       "1       0.68      9.8      Mid  \n",
       "2       0.65      9.8      Mid  \n",
       "3       0.58      9.8      Mid  \n",
       "4       0.56      9.4      Mid  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('winequality-red.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.4     0.7     0.      1.9     0.076  11.     34.      0.9978  3.51\n",
      "   0.56    9.4   ]\n",
      " [ 7.8     0.88    0.      2.6     0.098  25.     67.      0.9968  3.2\n",
      "   0.68    9.8   ]]\n",
      "['Mid' 'Mid' 'Mid' ... 'Mid' 'Mid' 'Mid']\n"
     ]
    }
   ],
   "source": [
    "# split data in inputs and classes\n",
    "inputs = df[df.columns[1:-1]].values\n",
    "classes = df[df.columns[-1]].values\n",
    "\n",
    "print(inputs[0:2,:])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bad' 'Good' 'Mid']\n",
      "[  63  217 1319]\n"
     ]
    }
   ],
   "source": [
    "# How many examples exist for each classe?\n",
    "unique, counts = np.unique(classes, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, as classes no conjunto de dados encontram-se extremamente desbalanceadas. Mais tarde, iremos propor métodos para tentar contornar esse problema, utilizando under e over sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = scale_data(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão em Conjunto de Treinamento e Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fim de que o conjunto de treinamento e o de teste mantenha a mesma proporção relativa de exemplos de cada classe, iremos realizar um split estratificado!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, classes, test_size=0.3, \n",
    "                                                    stratify=classes,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste de diferentes Arquiteturas de Camadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos testar diferentes arquiteturas de camadas. Primeiramente, iremos considerar uma única camada, variando o número de neurônios entre:\n",
    "- Metade do número de entradas: 5\n",
    "- Número de entradas: 11\n",
    "- Dobro do número de entradas: 22\n",
    "\n",
    "A seguir, iremos considerar arquiteturas com duas camadas, variando cada uma delas da mesma forma descrita acima!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 layer - 5 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.17097283090389398'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training\n",
    "N1 = 5\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(n_classes, N1))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.92%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     0     0   19  0.000000\n",
      "Good    0    15   50  0.230769\n",
      "Mid     0    13  383  0.967172\n",
      "Average accuracy per class: 39.93%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8291666666666667, 0.3993136493136493)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Layer - 11 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.15555351711307336'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 11\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(n_classes, N1))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.54%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     0     0   19  0.000000\n",
      "Good    0    25   40  0.384615\n",
      "Mid     5    15  376  0.949495\n",
      "Average accuracy per class: 44.47%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8354166666666667, 0.44470344470344475)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Layer - 22 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.15584908720846483'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 22\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(n_classes, N1))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.29%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     0   18  0.052632\n",
      "Good    0    28   37  0.430769\n",
      "Mid     5    25  366  0.924242\n",
      "Average accuracy per class: 46.92%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8229166666666666, 0.4692144113196745)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 5 Neurons - 5 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.17256073450057421'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 5\n",
    "N2 = 5\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.96%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     1   17  0.052632\n",
      "Good    0    24   41  0.369231\n",
      "Mid     2    16  378  0.954545\n",
      "Average accuracy per class: 45.88%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8395833333333333, 0.45880260090786407)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 5 Neurons - 11 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.17131903178171723'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 5\n",
    "N2 = 11\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.58%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     0     0   19  0.000000\n",
      "Good    0    28   37  0.430769\n",
      "Mid     1    17  378  0.954545\n",
      "Average accuracy per class: 46.18%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8458333333333333, 0.4617715617715618)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 5 Neurons - 22 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.18312974578027885'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 5\n",
    "N2 = 22\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.21%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     0     1   18  0.000000\n",
      "Good    0    37   28  0.569231\n",
      "Mid     0    24  372  0.939394\n",
      "Average accuracy per class: 50.29%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8520833333333333, 0.5028749028749029)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 11Neurons - 5 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.157005940957767'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 11\n",
    "N2 = 5\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.58%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     2     0   17  0.105263\n",
      "Good    0    24   41  0.369231\n",
      "Mid     3    13  380  0.959596\n",
      "Average accuracy per class: 47.80%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8458333333333333, 0.4780299622404886)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 11Neurons - 11 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.15924146409084203'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 11\n",
    "N2 = 11\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.00%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     0   18  0.052632\n",
      "Good    0    34   31  0.523077\n",
      "Mid     5    18  373  0.941919\n",
      "Average accuracy per class: 50.59%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.85, 0.5058758979811612)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 11Neurons - 22 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.17482560382191964'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 11\n",
    "N2 = 22\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.75%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     0   18  0.052632\n",
      "Good    0    24   41  0.369231\n",
      "Mid     5    14  377  0.952020\n",
      "Average accuracy per class: 45.80%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8375, 0.4579608500661132)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 22Neurons - 5 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.15610677816108867'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 22\n",
    "N2 = 5\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.12%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     1   17  0.052632\n",
      "Good    0    25   40  0.384615\n",
      "Mid     5    18  373  0.941919\n",
      "Average accuracy per class: 45.97%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.83125, 0.459722051827315)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 22Neurons - 11 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.1499731420710275'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 22\n",
    "N2 = 11\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.71%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     3     0   16  0.157895\n",
      "Good    0    24   41  0.369231\n",
      "Mid     8    18  370  0.934343\n",
      "Average accuracy per class: 48.72%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8270833333333333, 0.4871563134721029)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 22Neurons - 22 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.15165697320669774'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 22\n",
    "N2 = 22\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.21%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     0   18  0.052632\n",
      "Good    0    32   33  0.492308\n",
      "Mid     2    18  376  0.949495\n",
      "Average accuracy per class: 49.81%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8520833333333333, 0.49814474025000344)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Número de epochs usadas durante o Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos analisar como o número de epochs usadas durante o treinamento interfere na acurácia obtida no conjunto de testes! Para isso, vamos utilizar a arquitetura de rede que obteve melhor desempenho! A arquitetura de duas camadas com 5 e 22 neurônios teve o mesmo desempenho para a acurácia geral que considerando 22 neurônios nas camadas 1 e 2. Entretanto, como a acurácia por classe é melhor no primeiro caso e a rede é menor, facilitando o treinamento, utilizaremos esse modelos nos passos seguintes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = dict()\n",
    "best_N1 = 5\n",
    "best_N2 = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 200 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 200. Total Error = 0.19357673538234624'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_train = mlp.predict(X_train)\n",
    "predicted_test = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING SET ===\n",
      "Accuracy: 85.61%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     5     1   38  0.113636\n",
      "Good    0    64   88  0.421053\n",
      "Mid     2    32  889  0.963164\n",
      "Average accuracy per class: 49.93%\n",
      "=== TEST SET ===\n",
      "Accuracy: 83.33%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     0     0   19  0.000000\n",
      "Good    0    21   44  0.323077\n",
      "Mid     1    16  379  0.957071\n",
      "Average accuracy per class: 42.67%\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TRAINING SET ===\")\n",
    "evaluate(y_train, predicted_train)\n",
    "print(\"=== TEST SET ===\")\n",
    "accs[epochs] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 400 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.1723300286170253'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 400\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_train = mlp.predict(X_train)\n",
    "predicted_test = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING SET ===\n",
      "Accuracy: 87.58%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad    10     1   33  0.227273\n",
      "Good    0    81   71  0.532895\n",
      "Mid     7    27  889  0.963164\n",
      "Average accuracy per class: 57.44%\n",
      "=== TEST SET ===\n",
      "Accuracy: 84.79%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     3     0   16  0.157895\n",
      "Good    0    28   37  0.430769\n",
      "Mid     4    16  376  0.949495\n",
      "Average accuracy per class: 51.27%\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TRAINING SET ===\")\n",
    "evaluate(y_train, predicted_train)\n",
    "print(\"=== TEST SET ===\")\n",
    "accs[epochs] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 800 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 800. Total Error = 0.15944656209494504'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 800\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_train = mlp.predict(X_train)\n",
    "predicted_test = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING SET ===\n",
      "Accuracy: 88.03%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad    13     1   30  0.295455\n",
      "Good    0   109   43  0.717105\n",
      "Mid     9    51  863  0.934995\n",
      "Average accuracy per class: 64.92%\n",
      "=== TEST SET ===\n",
      "Accuracy: 82.92%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     2     1   16  0.105263\n",
      "Good    0    39   26  0.600000\n",
      "Mid     6    33  357  0.901515\n",
      "Average accuracy per class: 53.56%\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TRAINING SET ===\")\n",
    "evaluate(y_train, predicted_train)\n",
    "print(\"=== TEST SET ===\")\n",
    "accs[epochs] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 1000. Total Error = 0.15061523549796377'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_train = mlp.predict(X_train)\n",
    "predicted_test = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING SET ===\n",
      "Accuracy: 88.47%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad    11     1   32  0.250000\n",
      "Good    0   116   36  0.763158\n",
      "Mid     9    51  863  0.934995\n",
      "Average accuracy per class: 64.94%\n",
      "=== TEST SET ===\n",
      "Accuracy: 84.58%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     2     1   16  0.105263\n",
      "Good    0    41   24  0.630769\n",
      "Mid     3    30  363  0.916667\n",
      "Average accuracy per class: 55.09%\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TRAINING SET ===\")\n",
    "evaluate(y_train, predicted_train)\n",
    "print(\"=== TEST SET ===\")\n",
    "accs[epochs] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 2000. Total Error = 0.14620087345741456'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 2000\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted_train = mlp.predict(X_train)\n",
    "predicted_test = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING SET ===\n",
      "Accuracy: 85.97%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad    12     2   30  0.272727\n",
      "Good    1    96   55  0.631579\n",
      "Mid    23    46  854  0.925244\n",
      "Average accuracy per class: 60.98%\n",
      "=== TEST SET ===\n",
      "Accuracy: 81.67%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     3     1   15  0.157895\n",
      "Good    1    31   33  0.476923\n",
      "Mid     8    30  358  0.904040\n",
      "Average accuracy per class: 51.30%\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TRAINING SET ===\")\n",
    "evaluate(y_train, predicted_train)\n",
    "print(\"=== TEST SET ===\")\n",
    "accs[epochs] = evaluate(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{200: (0.8333333333333334, 0.42671587671587674), 400: (0.8479166666666667, 0.5127196390354286), 800: (0.8291666666666667, 0.5355927698032961), 1000: (0.8458333333333333, 0.5508996851102114), 2000: (0.8166666666666667, 0.5129527392685288)}\n"
     ]
    }
   ],
   "source": [
    "print(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        200       400       800       1000      2000\n",
      "Total accuracy      0.833333  0.847917  0.829167  0.845833  0.816667\n",
      "Accuracy per class  0.426716  0.512720  0.535593  0.550900  0.512953\n"
     ]
    }
   ],
   "source": [
    "epochs_accs = pd.DataFrame(accs)\n",
    "epochs_accs.index = [\"Total accuracy\", \"Accuracy per class\"]\n",
    "print(epochs_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repare que embora a Loss Function decresça para 800, 1000 e 2000 epochs, ao avaliar o modelo no conjunto de testes, há uma queda na acurácia! isso ilustra a ocorrência de um overfitting do modelo. Portanto, nas próximas etapas, iremos tomar 400 epochs como o máximo de iterações durante a fase de treinamento, uma vez que ela mantém um bom nível de acurácia total, enquanto eleva a acurácia por classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epochs = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate and Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixando o número de epochs em 800 e utilizando a arquitura de rede com melhor desempenho (2 camadas intermediárias:11 neurônios na primeira e 11 na segunda), vamos variar os parâmetros learning rate (eta) e momentum (alfa), e vamos observar como eles interferem no aprendizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.3 e Momentum = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.17852369217449351'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.12%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     0   18  0.052632\n",
      "Good    0    13   52  0.200000\n",
      "Mid     2     9  385  0.972222\n",
      "Average accuracy per class: 40.83%\n"
     ]
    }
   ],
   "source": [
    "eta = 0.3\n",
    "alpha = 0.3\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.3 e Momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.17005450714137374'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.92%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     2     1   16  0.105263\n",
      "Good    0    40   25  0.615385\n",
      "Mid     3    37  356  0.898990\n",
      "Average accuracy per class: 53.99%\n"
     ]
    }
   ],
   "source": [
    "eta = 0.3\n",
    "alpha = 0.5\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.3 e Momentum = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.16849678551862712'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.58%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     3     1   15  0.157895\n",
      "Good    0    34   31  0.523077\n",
      "Mid     4    23  369  0.931818\n",
      "Average accuracy per class: 53.76%\n"
     ]
    }
   ],
   "source": [
    "eta = 0.3\n",
    "alpha = 0.8\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.5 e Momentum = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.17178673034167954'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.17%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     0   18  0.052632\n",
      "Good    0    26   39  0.400000\n",
      "Mid     3    16  377  0.952020\n",
      "Average accuracy per class: 46.82%\n"
     ]
    }
   ],
   "source": [
    "eta = 0.5\n",
    "alpha = 0.3\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.5 e Momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.1660632096305141'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.79%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     2     0   17  0.105263\n",
      "Good    0    36   29  0.553846\n",
      "Mid     3    24  369  0.931818\n",
      "Average accuracy per class: 53.03%\n"
     ]
    }
   ],
   "source": [
    "eta = 0.5\n",
    "alpha = 0.5\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.5 e Momentum = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.16087797552456454'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.79%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     1   17  0.052632\n",
      "Good    0    21   44  0.323077\n",
      "Mid     1    10  385  0.972222\n",
      "Average accuracy per class: 44.93%\n"
     ]
    }
   ],
   "source": [
    "eta = 0.5\n",
    "alpha = 0.8\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.8 e Momentum = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.16368478056386876'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.58%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     0     0   19  0.000000\n",
      "Good    0    27   38  0.415385\n",
      "Mid     1    16  379  0.957071\n",
      "Average accuracy per class: 45.75%\n"
     ]
    }
   ],
   "source": [
    "eta = 0.8\n",
    "alpha = 0.3\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.8 e Momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.1656719841097967'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.58%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     2     1   16  0.105263\n",
      "Good    0    33   32  0.507692\n",
      "Mid     1    24  371  0.936869\n",
      "Average accuracy per class: 51.66%\n"
     ]
    }
   ],
   "source": [
    "eta = 0.8\n",
    "alpha = 0.5\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 0.8 e Momentum = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.16077643807005182'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.42%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     2     1   16  0.105263\n",
      "Good    0    28   37  0.430769\n",
      "Mid     1    15  380  0.959596\n",
      "Average accuracy per class: 49.85%\n"
     ]
    }
   ],
   "source": [
    "eta = 0.8\n",
    "alpha = 0.8\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate = 1 e Momentum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.16825300651265834'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.38%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     0     1   18  0.000000\n",
      "Good    0    27   38  0.415385\n",
      "Mid     0    18  378  0.954545\n",
      "Average accuracy per class: 45.66%\n"
     ]
    }
   ],
   "source": [
    "eta = 1\n",
    "alpha = 1\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0.3, 0.3): (0.83125, 0.40828460038986353),\n",
       " (0.3, 0.5): (0.8291666666666667, 0.5398792240897504),\n",
       " (0.3, 0.8): (0.8458333333333333, 0.5375966139124033),\n",
       " (0.5, 0.3): (0.8416666666666667, 0.4682172603225235),\n",
       " (0.5, 0.5): (0.8479166666666667, 0.5303091645196908),\n",
       " (0.5, 0.8): (0.8479166666666667, 0.4493102414155046),\n",
       " (0.8, 0.3): (0.8458333333333333, 0.45748510748510746),\n",
       " (0.8, 0.5): (0.8458333333333333, 0.5166080508185771),\n",
       " (0.8, 0.8): (0.8541666666666666, 0.49854278275330904),\n",
       " (1, 1): (0.84375, 0.4566433566433567)}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0.3       0.5       0.8        1\n",
      "0.3  0.831250  0.829167  0.845833      NaN\n",
      "0.5  0.841667  0.847917  0.847917      NaN\n",
      "0.8  0.845833  0.845833  0.854167      NaN\n",
      "1         NaN       NaN       NaN  0.84375\n"
     ]
    }
   ],
   "source": [
    "total_acc = pd.DataFrame()\n",
    "avg_acc = pd.DataFrame()\n",
    "\n",
    "for run, acc in accs.items():\n",
    "    total_acc.at[str(run[0]), str(run[1])] = acc[0]\n",
    "    avg_acc.at[str(run[0]), str(run[1])] = acc[1]\n",
    "    \n",
    "print(total_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0.3       0.5       0.8         1\n",
      "0.3  0.408285  0.539879  0.537597       NaN\n",
      "0.5  0.468217  0.530309  0.449310       NaN\n",
      "0.8  0.457485  0.516608  0.498543       NaN\n",
      "1         NaN       NaN       NaN  0.456643\n"
     ]
    }
   ],
   "source": [
    "print(avg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que, para esse problema considerado, aumentando o learning rate e o momentum, aumentamos a acurácia geral mas diminuímos a acurácia média por classe. Entretanto, tomando os valores igual a 1, a acurácia total obtida é menor que a atinginda com ambos valendo 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eta   = 0.8\n",
    "best_alpha = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variação do tamanho dos conjuntos de treinamento e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> Agora, vamos variar o tamanho dos conjuntos de treinamento e teste utilizando a melhor arquitetura encontrada acima e os melhores valores de learning rate e momentum. Utilizaremos incialmente 70% dos dados para treinamento, aumentando gradativamente esse valor até 90%.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 70% for training, 30% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.1633866935427087'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.17%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     2     1   16  0.105263\n",
      "Good    0    37   28  0.569231\n",
      "Mid     1    30  365  0.921717\n",
      "Average accuracy per class: 53.21%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8416666666666667, 0.5320703662808927)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, classes, test_size=0.3, \n",
    "                                                    stratify=classes,random_state=42)\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 75% for training, 25% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.17691930674123232'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.00%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     0     0   16  0.000000\n",
      "Good    0    16   38  0.296296\n",
      "Mid     0    10  320  0.969697\n",
      "Average accuracy per class: 42.20%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.84, 0.4219977553310887)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, classes, test_size=0.25, \n",
    "                                                    stratify=classes,random_state=42)\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80% for training, 20% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.1710581769015962'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.62%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     0     0   13  0.000000\n",
      "Good    0    15   28  0.348837\n",
      "Mid     0    21  243  0.920455\n",
      "Average accuracy per class: 42.31%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.80625, 0.42309725158562367)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, classes, test_size=0.2, \n",
    "                                                    stratify=classes,random_state=42)\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 85% for training, 15% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.1778925043554022'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.00%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     0     0    9  0.000000\n",
      "Good    0    14   19  0.424242\n",
      "Mid     0     8  190  0.959596\n",
      "Average accuracy per class: 46.13%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.85, 0.4612794612794613)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, classes, test_size=0.15, \n",
    "                                                    stratify=classes,random_state=42)\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 90% for training, 10% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.171274089212619'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.25%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     0     0    6  0.000000\n",
      "Good    0    10   12  0.454545\n",
      "Mid     0     4  128  0.969697\n",
      "Average accuracy per class: 47.47%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8625, 0.47474747474747475)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, classes, test_size=0.1, \n",
    "                                                    stratify=classes,random_state=42)\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 95% for training, 5% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.1943626528283032'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.75%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     0     0    3  0.000000\n",
      "Good    0     4    7  0.363636\n",
      "Mid     0     3   63  0.954545\n",
      "Average accuracy per class: 43.94%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8375, 0.43939393939393945)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, classes, test_size=0.05, \n",
    "                                                    stratify=classes,random_state=42)\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir das classificações realizadas, é possível observar que utilizando 70% dos dados para treinamento e 30% dos dados para teste, obtivemos a melhor acurácia média por classe, enquanto que utilizando 90% dos dados para treinamento e 10% para teste, foi obtida a melhor acurácia geral.\n",
    "\n",
    "Entretanto, como utilizando 90% para treinamento, resulta-se em um conjunto de testes com apenas 3 exemplares da classe 'Bad', a divisão entre 70% e 30% será considerada como a que apresentou melhores resultados e será utilizada para os próximos testes daqui em diante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_size = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisitando o Pré Processamento dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar nas classificações acima, na base de dados considerada, há poucos exemplos da classe Bad. Como consequência, como estamos dando o mesmo peso para um erro cometido em qualquer classe, a rede acaba por classificar a maioria dos exemplos como sendo pertecentes à classe com maior número de exemplos ('Mid').\n",
    "\n",
    "A seguir, vamos considerar uma base de dados com um número balanceado entre as classes. Para tal, vamos tomar o tamanho da menor classe e escolher exemplos aleatórios das demais classes para igualar esse número.\n",
    "\n",
    "Também vamos considerar uma base de dados com exemplos artificiais que serão criados para igualar o número de exemplos da menor classe com o número de exemplos da classe intermediária. Não igualaremos o número de exemplares pela classe de maior cardinalidade, uma vez que, no mundo real, é normal que haja mais itens de qualidade intermediária que itens de qualidade ruim ou boa. \n",
    "\n",
    "Para melhor comparar as base de dados e não ser influenciado pela aleatoriedade com a qual os conjuntos de treinamento e teste são escolhidos, iremos avaliar os modelos utilizando CrossValidation Stratified com 10 folds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('winequality-red.csv')\n",
    "inputs = df[df.columns[1:-1]].values\n",
    "classes = df[df.columns[-1]].values\n",
    "\n",
    "unique, counts = np.unique(classes, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the smallest amount of examples, amongts all classes in the dataset\n",
    "examples = min(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly chooses 63 indexes of examples from each class \n",
    "bad_indices = np.random.choice(np.where(classes == 'Bad')[0], examples)\n",
    "good_indices = np.random.choice(np.where(classes == 'Good')[0], examples)\n",
    "mid_indices = np.random.choice(np.where(classes == 'Mid')[0], examples)\n",
    "\n",
    "# stores the chosen examples from 'Bad' class, as well as the same amount of labels from it\n",
    "under_sampled_examples = scaled_inputs[bad_indices]\n",
    "under_sampled_classes = classes[bad_indices]\n",
    "\n",
    "print(under_sampled_classes)\n",
    "\n",
    "# stores the chosen examples from 'Good' class, as well as the same amount of labels from it\n",
    "under_sampled_examples = np.append(under_sampled_examples,scaled_inputs[good_indices], axis=0)\n",
    "under_sampled_classes = np.append(under_sampled_classes, classes[good_indices], axis=0)\n",
    "\n",
    "# stores the chosen examples from 'Mid' class, as well as the same amount of labels from it \n",
    "under_sampled_examples = np.append(under_sampled_examples,scaled_inputs[mid_indices], axis=0)\n",
    "under_sampled_classes = np.append(under_sampled_classes, classes[mid_indices], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the new dataset in a training set and a test set, using 70% to 30% proportion\n",
    "X_train, X_test, y_train, y_test = train_test_split(under_sampled_examples, under_sampled_classes, \n",
    "                                                    test_size=best_test_size, \n",
    "                                                    stratify=under_sampled_classes,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Cross Validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the best architecture, number of epochs and alpha and eta values, all previously obtained,\n",
    "# for classifying the new examples, without using Stratified K-Fold metrics\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> Como é possível observar, a acurácia obtida na classificação utilizando undersampling piorou em cerca de 10% em comparação com a classificação realizada utilizando a base de dados original. Porém, a acurácia média por classes melhorou consideravelmente, em cerca de 20%, quando comparada à melhor acurácia por classe obtida anteriormente, de aproximadamente 53%. Isso se deve ao fato de que a quantidade de exemplos disponíveis para cada classe é a mesma, de forma que a classificação não se torna enviesada, favorecendo os exemplos da classe majoritária em detrimento das outras.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Cross Validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the best architecture, number of epochs and alpha and eta values, all previously obtained,\n",
    "# for classifying the new examples, using Stratified K-Fold metrics, for K = 10.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "accuracies = list()\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "\n",
    "X = over_sampled_examples\n",
    "y = over_sampled_classes\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(train_index, test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "              print_status=False, shuffle=True)\n",
    "\n",
    "    predicted  = mlp.predict(X_test)\n",
    "    acc = evaluate(y_test, predicted, display=False)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "mean_acc = np.array([])\n",
    "mean_classes_acc = np.array([])\n",
    "\n",
    "for i in range(len(accuracies)):\n",
    "    mean_acc = np.append(mean_acc, accuracies[i][0])\n",
    "    mean_classes_acc = np.append(mean_classes_acc, accuracies[i][1])\n",
    "\n",
    "print(\"Average accuracy: \" + '{:.2f}'.format(np.mean(mean_acc)*100) + \"%\")\n",
    "print(\"Average accuracy per class: \" + '{:.2f}'.format(np.mean(mean_classes_acc)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on the complete dataset, trained with the undersampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted  = mlp.predict(X_test)\n",
    "acc = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar o over-sampling dos dados, iremos utlizar o Método SMOTE(Synthetic Minority Over-sampling Technique) presente na biblioteca imbalanced-learn (https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html#imblearn.over_sampling.SMOTE).\n",
    "\n",
    "Nesse método, cria-se exemplos artificiais da seguinte maneira:\n",
    "    - Escolhe-se um exemplo da classe minoritária\n",
    "    - Toma-se k vizinhos mais próximos dele da mesma classe\n",
    "    - Determina-se o vetor entre o exemplo e seus vizinhos\n",
    "    - Cria-se um novo exemplo multiplicando esse vetor por um número aleatório entre 0 e 1\n",
    "    \n",
    "Além desse técnica, poderímos duplicar, triplicar e assim por diante, exemplos das classes minoritárias realizando uma amostragem aleatória com reposição nessas classes. Entretanto, como terminaríamos com exemplos duplicados nas classes minoritárias, optamos pela criação de exemplos artificiais através da técnica SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# specify the class targeted by the resampling. The number of samples in the different classes will be equalized\n",
    "# 'not majority': resample all classes but the majority class\n",
    "sm = SMOTE(sampling_strategy='not majority')\n",
    "\n",
    "# resample the dataset, using parameters: matrix containing the data which have to be sampled and corresponding \n",
    "# label for each sample in matrix\n",
    "# 'over_sampled_dfX': The array containing the resampled data\n",
    "# 'over_sampled_dfY': The corresponding label of over_sampled_dfX\n",
    "over_sampled_dfX, over_sampled_dfY = sm.fit_sample(df.drop('category', axis=1), df['category'])\n",
    "\n",
    "# new df containing the resampled data\n",
    "over_sampled_df = pd.concat([pd.DataFrame(over_sampled_dfX), pd.DataFrame(over_sampled_dfY)], axis=1)\n",
    "over_sampled_df.columns = df.columns\n",
    "\n",
    "#over_sampled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação dos Dados\n",
    "over_sampled_inputs = over_sampled_df[over_sampled_df.columns[1:-1]].values\n",
    "over_sampled_classes = over_sampled_df[over_sampled_df.columns[-1]].values\n",
    "\n",
    "print(over_sampled_inputs[0:2,:])\n",
    "print(over_sampled_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_sampled_scaled_inputs = scale_data(over_sampled_inputs)\n",
    "over_sampled_scaled_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the new dataset in a training set and a test set, using 70% to 30% proportion\n",
    "X_train, X_test, y_train, y_test = train_test_split(over_sampled_scaled_inputs, over_sampled_classes, \n",
    "                                                    test_size=best_test_size, \n",
    "                                                    stratify=over_sampled_classes,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Cross Validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the best architecture, number of epochs and alpha and eta values, all previously obtained,\n",
    "# for classifying the new examples, without using Stratified K-Fold metrics\n",
    "random.seed(0)\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Cross Validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the best architecture, number of epochs and alpha and eta values, all previously obtained,\n",
    "# for classifying the new examples, using Stratified K-Fold metrics, for K = 10.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "accuracies = list()\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "\n",
    "X = over_sampled_scaled_inputs\n",
    "y = over_sampled_classes\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(train_index, test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=200, \n",
    "              print_status=False, shuffle=True)\n",
    "\n",
    "    predicted  = mlp.predict(X_test)\n",
    "    acc = evaluate(y_test, predicted, display=False)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "mean_acc = np.array([])\n",
    "mean_classes_acc = np.array([])\n",
    "\n",
    "for i in range(len(accuracies)):\n",
    "    mean_acc = np.append(mean_acc, accuracies[i][0])\n",
    "    mean_classes_acc = np.append(mean_classes_acc, accuracies[i][1])\n",
    "\n",
    "print(\"Average accuracy: \" + '{:.2f}'.format(np.mean(mean_acc)*100) + \"%\")\n",
    "print(\"Average accuracy per class: \" + '{:.2f}'.format(np.mean(mean_classes_acc)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on the complete dataset, trained with the oversampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, classes, test_size=0.3, \n",
    "                                                    stratify=classes,random_state=42)\n",
    "\n",
    "predicted  = mlp.predict(X_test)\n",
    "acc = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui será feito o treinamento e teste utilizando o conjunto de dados completo, sem utilização de undersampling ou oversampling, com o método de validação cruzada Stratified K-Fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the best architecture, number of epochs and alpha and eta values, all previously obtained,\n",
    "# for classifying the new examples, using Stratified K-Fold metrics, for K = 10.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "accuracies = list()\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "\n",
    "X = scaled_inputs\n",
    "y = classes\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "              print_status=False, shuffle=True)\n",
    "\n",
    "    predicted  = mlp.predict(X_test)\n",
    "    acc = evaluate(y_test, predicted, display=False)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "mean_acc = np.array([])\n",
    "mean_classes_acc = np.array([])\n",
    "\n",
    "for i in range(len(accuracies)):\n",
    "    mean_acc = np.append(mean_acc, accuracies[i][0])\n",
    "    mean_classes_acc = np.append(mean_classes_acc, accuracies[i][1])\n",
    "\n",
    "print(\"Average accuracy: \" + '{:.2f}'.format(np.mean(mean_acc)*100) + \"%\")\n",
    "print(\"Average accuracy per class: \" + '{:.2f}'.format(np.mean(mean_classes_acc)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
