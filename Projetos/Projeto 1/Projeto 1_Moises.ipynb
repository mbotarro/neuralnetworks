{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.utils import shuffle as shuffle_data\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# Layer represents a MLP Layer\n",
    "# It has two main properties:\n",
    "#      - a weigth matrix containing the weights of the layer's neurons. Each line represents a neuron and \n",
    "#        the columns represent its corresponding weights\n",
    "#      - a bias vector, containing the neurons's bias\n",
    "# Since during the backpropagation we need to compute the weights variation using the old ones, the \n",
    "# updated_weights and updated_bias properties store the new values until the update method is called\n",
    "class Layer:\n",
    "    # Create a new Layer with 'size' neurons, each one linked to 'inputs_size' inputs\n",
    "    def __init__(self, size, inputs_size):\n",
    "        self.size = size\n",
    "        self.inputs_size = inputs_size\n",
    "        self.weights = np.array([[random.uniform(-0.1, 0.1) for j in range(inputs_size)] for i in range(size)])\n",
    "        self.bias = np.array([random.uniform(-0.1,0.1) for i in range(size)])\n",
    "        \n",
    "        self.d_weights_current = np.zeros((size, inputs_size))\n",
    "        self.d_bias_current = np.zeros(size)\n",
    "        self.d_weights_old = np.zeros((size, inputs_size))\n",
    "        self.d_bias_old = np.zeros(size)\n",
    "    \n",
    "    # update updates the weights and bias matrices with the values stored in the updated ones\n",
    "    def update(self, eta, alpha):\n",
    "        #self.weights = np.copy(self.updated_weights)\n",
    "        #self.bias = np.copy(self.updated_bias)\n",
    "        \n",
    "        self.weights = self.weights + eta*self.d_weights_current + alpha*self.d_weights_old \n",
    "        self.bias = self.bias + eta*self.d_bias_current + alpha*self.d_bias_old\n",
    "        \n",
    "        self.d_weights_old = self.d_weights_current\n",
    "        self.d_bias_old = self.d_bias_current\n",
    "        \n",
    "    # description prints a layer description\n",
    "    def description(self):\n",
    "        print(\"Layer Info\")\n",
    "        print(\"Weights: \\n\", self.weights)\n",
    "        print(\"Bias: \\n \", self.bias)\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0/(1.0+ math.exp(-x))\n",
    "\n",
    "logistic_vec = np.vectorize(logistic)\n",
    "\n",
    "def logistic_derivate(x):\n",
    "    return x*(1.0-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    # MLP creation. One might pass the MLP layers as parameters or add them later using the add_layer method.\n",
    "    # The classification parameter defines if the MLP will be used for a classification or regression problem\n",
    "    def __init__(self, *layers, classifier=True):\n",
    "        self.classifier = classifier\n",
    "        if classifier:\n",
    "            # Map each class label to a vector with a single 1\n",
    "            # Ex: Class 0 -> [1,0]\n",
    "            #     Class 1 -> [0,1]\n",
    "            self.class_mapping = dict()  \n",
    "            # Unmap each class vector to the corresponding class label\n",
    "            # Ex: [1,0] -> Class 0 \n",
    "            #     [0,1] -> Class 1\n",
    "            self.class_unmapping = dict()\n",
    "            \n",
    "        self.layers = list()\n",
    "        for layer in layers:\n",
    "            self.add_layer(layer)\n",
    "    \n",
    "    # Shortcut to create a classifier MLP\n",
    "    @classmethod\n",
    "    def MLPClassifier(cls, *layers):\n",
    "        return cls(classifier=True, *layers)   \n",
    "    \n",
    "    # Shortcut to create a regressor MLP\n",
    "    @classmethod\n",
    "    def MLPRegressor(cls, *layers):\n",
    "        return cls(classifier=False, *layers)\n",
    "    \n",
    "    # add_layer adds a new layer on the MLP. It verifies whether or not the new layer is compatible with the MLP\n",
    "    def add_layer(self, layer):\n",
    "        # If there's already a layer in the MLP, verify if the new layer is compatible\n",
    "        if len(self.layers) > 0:\n",
    "            if layer.inputs_size != self.layers[-1].size:\n",
    "                print(\"The new layer is incompatible with the MLP\")\n",
    "                print(\"Please, use a layer where each neuron has the same amount of inputs as the number\" \\\n",
    "                     \"of neurons in the MLP last layer\")\n",
    "        \n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    # description prints the info about the MLP layers\n",
    "    def description(self):\n",
    "        print(\"MLP Classifier?: \", self.classifier)\n",
    "        print(\"-------------------------\")\n",
    "        print(\"MLP Info:\")\n",
    "        for layer, i in zip(self.layers, range(len(self.layers))):\n",
    "            print(\"--- Layer: %d ---\" % i)\n",
    "            layer.description()\n",
    "            \n",
    "    # __get_class_mapping gets the class labels in the classes list and builds the mapping dicionaries\n",
    "    # class_mapping and class_unmapping\n",
    "    def __get_class_mapping(self, classes):\n",
    "        class_labels = np.unique(classes)\n",
    "        \n",
    "        for c in range(len(class_labels)):\n",
    "            class_label = class_labels[c]\n",
    "            class_vector = np.zeros(len(class_labels))\n",
    "            class_vector[c] = 1\n",
    "    \n",
    "            self.class_mapping[class_label] = class_vector\n",
    "            \n",
    "            # We can't use a list as a hash key. So transform it into a tuple\n",
    "            self.class_unmapping[tuple(class_vector)] = class_label\n",
    "        \n",
    "    # __convert_class_labels_to_vectors converts a list with class labels to a list with \n",
    "    # vectors that maps each class label\n",
    "    def __convert_class_labels_to_vectors(self, class_labels):\n",
    "        return [self.class_mapping[c] for c in class_labels]\n",
    "    \n",
    "    # __convert_class_vectors_to_labels converts a list with class vectors to a list with \n",
    "    # the corresponding class labels\n",
    "    def __convert_class_vectors_to_labels(self, class_vectors):\n",
    "        return [self.class_unmapping[tuple(class_vector)] for class_vector in class_vectors]\n",
    "        \n",
    "        \n",
    "    # fast_forward computes the ouput for a given input vector\n",
    "    def fast_forward(self,input_v):\n",
    "        # We need to store each layer input in order to perform the backpropagation\n",
    "        self.inputs = list()\n",
    "    \n",
    "        # The input is applied in a layer weights matrix and the bias is added in the result\n",
    "        # Then, the logistic function is applied to each layer neuron result\n",
    "        # For a layer, we have a final output vector where each component i represents the output\n",
    "        # of the neuron i\n",
    "        for l in range(len(self.layers)-1):\n",
    "            layer = self.layers[l]\n",
    "            self.inputs.append(input_v)\n",
    "            output = logistic_vec(layer.weights @ input_v + layer.bias)\n",
    "            \n",
    "            # The output of the current layer is the input of the next one\n",
    "            input_v = output\n",
    "            \n",
    "        # Last layer\n",
    "        self.inputs.append(input_v)\n",
    "        layer = self.layers[-1]\n",
    "        output = layer.weights @ input_v + layer.bias\n",
    "        \n",
    "        # If it's a classification problem, apply the softmax function on the output of the last layer\n",
    "        # It it's a regression problem, the activation function of the last layer is the identity function\n",
    "        if self.classifier:\n",
    "            #print(\"OUTPUT: \", output)\n",
    "            #output = np.exp(output)/sum(np.exp(output))\n",
    "            output = logistic_vec(output)\n",
    "            \n",
    "        else:\n",
    "            output = logistic_vec(output)\n",
    "          \n",
    "        return output\n",
    "    \n",
    "    # train trains the MLP using the examples passed in the samples parameter\n",
    "    # The expected output for each example must be passed in the classes parameter;\n",
    "    # eta represents the MLP learning rate;\n",
    "    # tol represents the error tolerance. The MLP is trained until the cumulative squared error for all example\n",
    "    #     is less than the tol value\n",
    "    # print_status prints the output for each example during the training phase\n",
    "    def train(self, samples, classes, eta=0.5, alpha=0, tol=1e-2, epoch_max=2000, \n",
    "              print_status=False, shuffle=True):\n",
    "        # Map the class labels to output vectors if it's a classification problem\n",
    "        if self.classifier:\n",
    "            self.__get_class_mapping(classes)\n",
    "            classes = self.__convert_class_labels_to_vectors(classes)\n",
    "                \n",
    "        error = tol\n",
    "        new_error = 3*tol\n",
    "        epoch = 0\n",
    "        \n",
    "        # The training stops when the max number of epochs is reached or the Kramer and Sangiovanni-Vicentelly\n",
    "        # criteria is valid. According to it, we can consider that the BP converged when the average mean squared\n",
    "        # error is less than a given tolerance\n",
    "        while (abs(new_error - error) > tol and epoch < epoch_max):\n",
    "            epoch += 1\n",
    "            error = 0\n",
    "            new_error = 0\n",
    "            \n",
    "            if shuffle:\n",
    "                samples, classes = shuffle_data(samples, classes)\n",
    "            \n",
    "            for input_v, t in zip(samples, classes):  \n",
    "                # ---- Compute the output for the given input vector ----\n",
    "                output = self.fast_forward(input_v)\n",
    "                \n",
    "                # Compute the mean squared error before the backpropagation\n",
    "                error_sample = pow((np.array(t)-np.array(output)),2)\n",
    "                # We need to sum the error of each component when the output is a vector\n",
    "                error += sum(error_sample)/len(samples)\n",
    "                \n",
    "                if (print_status == True):\n",
    "                    print(\"\\ttraining example: %s from class %s\" % (input_v, t), end = \" \")\n",
    "                    print(\"y = \", output)\n",
    "\n",
    "                    \n",
    "                # ---- Backpropagation ----\n",
    "                # Compute the new weights of each layer\n",
    "                # Remark: the udpated weights are stored as a layer property and the layer is updated once \n",
    "                # the backpropagation is finished\n",
    "                # It's necessary to do so in order to compute the delta value for the inner layers. We need \n",
    "                # to use the weights that caused the error to compute the delta instead of the updated weights\n",
    "                for l in reversed(range(len(self.layers))): # Traverse the layers in reversed order\n",
    "                    layer = self.layers[l]\n",
    "             \n",
    "                    deltas = list()\n",
    "                    # Compute the delta for each layer neuron n\n",
    "                    for n in range(len(layer.weights)):\n",
    "                        # Last Layer\n",
    "                        if l == (len(self.layers)-1):\n",
    "                            delta = (t[n]-output[n])*logistic_derivate(output[n])\n",
    "                            \n",
    "                        # Inner Layer\n",
    "                        else:\n",
    "                            # output of the current layer is the input of the next one\n",
    "                            neuron_output = self.inputs[l+1][n]\n",
    "                            # weights of each neuron output\n",
    "                            errors_weights = self.layers[l+1].weights[:,n]\n",
    "                            \n",
    "                            delta = np.dot(delta_next_layer,errors_weights)*logistic_derivate(neuron_output)\n",
    "                              \n",
    "                        # Computes the weights and bias variation for the neuron n\n",
    "                        for w in range(len(layer.weights[n])):\n",
    "                            layer.d_weights_current[n][w] = delta*self.inputs[l][w]\n",
    "                        layer.d_bias_current[n] = delta*1 # bias input = 1\n",
    "                        \n",
    "                        #for w in range(len(layer.weights[n])):\n",
    "                        #    layer.updated_weights[n][w] = layer.weights[n][w] + eta*delta*self.inputs[l][w]\n",
    "                        #layer.updated_bias[n] = layer.bias[n] + (eta*delta*1) # bias input = 1\n",
    "\n",
    "                        # Store the neuron delta\n",
    "                        deltas.append(delta)\n",
    "                    \n",
    "                    # The neurons' delta of the current layer will be used to compute the deltas of the \n",
    "                    # next inner layer\n",
    "                    delta_next_layer = np.array(deltas)\n",
    "                     \n",
    "                # Once the backpropagation is finished for the current example, update all the weigths and bias\n",
    "                for layer in self.layers:\n",
    "                    layer.update(eta, alpha)\n",
    "                    \n",
    "                # Compute the new error mean squared error\n",
    "                output = self.fast_forward(input_v)\n",
    "                error_sample = pow((np.array(t)-np.array(output)),2)\n",
    "                #print(\"error sample: \",error_sample)\n",
    "                new_error += sum(error_sample)/len(samples)\n",
    "            \n",
    "            # End of a epoch\n",
    "            if epoch%1 == 0: # Print status only after each 100 iterations \n",
    "                clear_output(wait=True)\n",
    "                display(\"End of epoch \" + str(epoch) + \". Total Error = \" + str(new_error))\n",
    "        \n",
    "        # End of training         \n",
    "        clear_output(wait=True)\n",
    "        display(\"End of epoch \" + str(epoch) + \". Total Error = \" + str(new_error))\n",
    "        \n",
    "    # predicts gets a list of input samples and returns a list with the predicted outputs\n",
    "    def predict(self, samples):\n",
    "        outputs = list()\n",
    "        for input_v in samples:\n",
    "            probs = self.fast_forward(input_v)\n",
    "            \n",
    "            if self.classifier:\n",
    "                class_pos = np.argmax(probs)\n",
    "                output = np.zeros(len(probs))\n",
    "                output[class_pos] = 1\n",
    "            \n",
    "                #outputs.append(self.class_unmapping[tuple(output)])\n",
    "                outputs.append(output)\n",
    "                \n",
    "            else:\n",
    "                outputs.append(probs)\n",
    "    \n",
    "        if self.classifier:\n",
    "            return self.__convert_class_vectors_to_labels(outputs)\n",
    "        \n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 1118. Total Error = 0.004833229768627075'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(0)\n",
    "\n",
    "samples = [[0,0], [1,0], [1,1], [0,1]]\n",
    "classes = [[1,0],[0,1],[1,0],[0,1]]\n",
    "\n",
    "mlp = MLP.MLPClassifier(Layer(5,2), Layer(2,5))\n",
    "mlp.train([[0,0], [1,0], [1,1], [0,1]], [0,1,0,1], eta=0.5, alpha=0.8, tol=1e-4, print_status=False)\n",
    "predicted = mlp.predict(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.predict([[0,0],[0,1],[1,0],[1,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data transforms data in order to all points have mean 0 and variance 1\n",
    "def normalize_data(data):\n",
    "    normalized_columns = list()\n",
    "    for c in range(len(data[0])):\n",
    "        col = data[:,c]\n",
    "        normalized_columns.append((col - np.mean(col))/np.std(col))\n",
    "\n",
    "    return np.array(normalized_columns).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_data transforms data in order to all points be in the interval [0,1]\n",
    "def scale_data(data):\n",
    "    normalized_columns = list()\n",
    "    for c in range(len(data[0])):\n",
    "        col = data[:,c]\n",
    "        normalized_columns.append((col-np.min(col))/(np.max(col)-np.min(col)))\n",
    "\n",
    "    return np.array(normalized_columns).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "0           0            7.4              0.70         0.00             1.9   \n",
       "1           1            7.8              0.88         0.00             2.6   \n",
       "2           2            7.8              0.76         0.04             2.3   \n",
       "3           3           11.2              0.28         0.56             1.9   \n",
       "4           4            7.4              0.70         0.00             1.9   \n",
       "\n",
       "   chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
       "0      0.076                 11.0                  34.0   0.9978  3.51   \n",
       "1      0.098                 25.0                  67.0   0.9968  3.20   \n",
       "2      0.092                 15.0                  54.0   0.9970  3.26   \n",
       "3      0.075                 17.0                  60.0   0.9980  3.16   \n",
       "4      0.076                 11.0                  34.0   0.9978  3.51   \n",
       "\n",
       "   sulphates  alcohol category  \n",
       "0       0.56      9.4      Mid  \n",
       "1       0.68      9.8      Mid  \n",
       "2       0.65      9.8      Mid  \n",
       "3       0.58      9.8      Mid  \n",
       "4       0.56      9.4      Mid  "
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('winequality-red.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.4     0.7     0.      1.9     0.076  11.     34.      0.9978  3.51\n",
      "   0.56    9.4   ]\n",
      " [ 7.8     0.88    0.      2.6     0.098  25.     67.      0.9968  3.2\n",
      "   0.68    9.8   ]]\n",
      "['Mid' 'Mid' 'Mid' ... 'Mid' 'Mid' 'Mid']\n"
     ]
    }
   ],
   "source": [
    "# Separação dos Dados\n",
    "inputs = df[df.columns[1:-1]].values\n",
    "classes = df[df.columns[-1]].values\n",
    "\n",
    "print(inputs[0:2,:])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bad', 'Good', 'Mid'], dtype=object)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 10. Total Error = 0.23409733106997108'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-a228602aa9a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-381fc5afeea5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, samples, classes, eta, alpha, tol, epoch_max, print_status, shuffle)\u001b[0m\n\u001b[1;32m    175\u001b[0m                         \u001b[0;31m# Computes the weights and bias variation for the neuron n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_weights_current\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m                         \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_bias_current\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;31m# bias input = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(classes))\n",
    "N = len(inputs[0])\n",
    "mlp = MLP(Layer(2*N, N), Layer(n_classes, 2*N))\n",
    "\n",
    "mlp.train(inputs, classes, eta=0.5, alpha=1.2, tol=1e-2, print_status=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUIDADO: Problema. math range na função math.exp -> saturação da saída dos neurônios provavelmente devido ao fato dos atributos não estarem normalizados entre 0 e 1.\n",
    "\n",
    "Outro problema: Os atributos não possuem mesma variância."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pré-Processamento: Colocar mais exemplos para que todas classes fiquem com mesma quantidade de exemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(classes, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bad', 'Good', 'Mid'], dtype=object)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  63,  217, 1319])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.52835961,  0.96187667, -1.39147228, ...,  1.28864292,\n",
       "        -0.57920652, -0.96024611],\n",
       "       [-0.29854743,  1.96744245, -1.39147228, ..., -0.7199333 ,\n",
       "         0.1289504 , -0.58477711],\n",
       "       [-0.29854743,  1.29706527, -1.18607043, ..., -0.33117661,\n",
       "        -0.04808883, -0.58477711],\n",
       "       ...,\n",
       "       [-1.1603431 , -0.09955388, -0.72391627, ...,  0.70550789,\n",
       "         0.54204194,  0.54162988],\n",
       "       [-1.39015528,  0.65462046, -0.77526673, ...,  1.6773996 ,\n",
       "         0.30598963, -0.20930812],\n",
       "       [-1.33270223, -1.21684919,  1.02199944, ...,  0.51112954,\n",
       "         0.01092425,  0.54162988]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized = normalize_data(inputs)\n",
    "normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compara with SKLearn standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.52835961  0.96187667 -1.39147228 ...  1.28864292 -0.57920652\n",
      "  -0.96024611]\n",
      " [-0.29854743  1.96744245 -1.39147228 ... -0.7199333   0.1289504\n",
      "  -0.58477711]\n",
      " [-0.29854743  1.29706527 -1.18607043 ... -0.33117661 -0.04808883\n",
      "  -0.58477711]\n",
      " ...\n",
      " [-1.1603431  -0.09955388 -0.72391627 ...  0.70550789  0.54204194\n",
      "   0.54162988]\n",
      " [-1.39015528  0.65462046 -0.77526673 ...  1.6773996   0.30598963\n",
      "  -0.20930812]\n",
      " [-1.33270223 -1.21684919  1.02199944 ...  0.51112954  0.01092425\n",
      "   0.54162988]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit_transform(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 500. Total Error = 0.14032719381344355'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = len(inputs[0])\n",
    "mlp = MLP(Layer(N, N), Layer(n_classes, N))\n",
    "\n",
    "random.seed(0)\n",
    "mlp.train(normalized, classes, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=500, print_status=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good'"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8974358974358975"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(np.array(classes), np.array(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  13,    3,   47],\n",
       "       [   1,  118,   98],\n",
       "       [   0,   15, 1304]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(classes, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 500. Total Error = 0.1184189503961069'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = len(inputs[0])\n",
    "mlp = MLP(Layer(N, N), Layer(n_classes, N))\n",
    "\n",
    "random.seed(0)\n",
    "mlp.train(normalized, classes, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=500, print_status=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9299562226391495"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mlp.predict(normalized)\n",
    "accuracy_score(np.array(classes), np.array(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  27,    3,   33],\n",
       "       [   0,  163,   54],\n",
       "       [   3,   19, 1297]])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(classes, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle a cada iteração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 500. Total Error = 0.1184189503961069'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = len(inputs[0])\n",
    "mlp = MLP(Layer(N, N), Layer(n_classes, N))\n",
    "\n",
    "random.seed(0)\n",
    "mlp.train(normalized, classes, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=500, print_status=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9299562226391495"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mlp.predict(normalized)\n",
    "accuracy_score(np.array(classes), np.array(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  27,    3,   33],\n",
       "       [   0,  163,   54],\n",
       "       [   3,   19, 1297]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(classes, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bad' 'Good' 'Mid']\n",
      "[  63  217 1319]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('winequality-red.csv')\n",
    "df.head(5)\n",
    "\n",
    "inputs = df[df.columns[1:-1]].values\n",
    "classes = df[df.columns[-1]].values\n",
    "\n",
    "normalized = data_normalization(inputs)\n",
    "\n",
    "unique, counts = np.unique(classes, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = min(counts)\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n"
     ]
    }
   ],
   "source": [
    "bad_indices = np.random.choice(np.where(classes == 'Bad')[0], examples)\n",
    "good_indices = np.random.choice(np.where(classes == 'Good')[0], examples)\n",
    "mid_indices = np.random.choice(np.where(classes == 'Mid')[0], examples)\n",
    "\n",
    "under_sampled_examples = normalized[bad_indices]\n",
    "under_sampled_classes = classes[bad_indices]\n",
    "\n",
    "under_sampled_examples = np.append(under_sampled_examples,normalized[good_indices], axis=0)\n",
    "under_sampled_classes = np.append(under_sampled_classes, classes[good_indices], axis=0)\n",
    "\n",
    "under_sampled_examples = np.append(under_sampled_examples,normalized[mid_indices], axis=0)\n",
    "under_sampled_classes = np.append(under_sampled_classes, classes[mid_indices], axis=0)\n",
    "\n",
    "len(under_sampled_examples)\n",
    "print(len(under_sampled_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n classes: 3\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(classes))\n",
    "print(\"n classes:\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 220. Total Error = 0.08730661839919339'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(1)\n",
    "N = len(inputs[0])\n",
    "mlp = MLP.MLPClassifier(Layer(N, N), Layer(n_classes, N))\n",
    "\n",
    "mlp.train(under_sampled_examples, under_sampled_classes, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=500, print_status=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(under_sampled_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9365079365079365"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(under_sampled_classes, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[60,  0,  3],\n",
       "       [ 0, 59,  4],\n",
       "       [ 0,  5, 58]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(under_sampled_classes, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bad' 'Good' 'Mid']\n",
      "[  63  217 1319]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('winequality-red.csv')\n",
    "df.head(5)\n",
    "\n",
    "inputs = df[df.columns[1:-1]].values\n",
    "classes = df[df.columns[-1]].values\n",
    "\n",
    "scaled = scale_data(inputs)\n",
    "\n",
    "unique, counts = np.unique(classes, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24778761, 0.39726027, 0.        , ..., 0.60629921, 0.13772455,\n",
       "        0.15384615],\n",
       "       [0.28318584, 0.52054795, 0.        , ..., 0.36220472, 0.20958084,\n",
       "        0.21538462],\n",
       "       [0.28318584, 0.43835616, 0.04      , ..., 0.40944882, 0.19161677,\n",
       "        0.21538462],\n",
       "       ...,\n",
       "       [0.15044248, 0.26712329, 0.13      , ..., 0.53543307, 0.25149701,\n",
       "        0.4       ],\n",
       "       [0.11504425, 0.35958904, 0.12      , ..., 0.65354331, 0.22754491,\n",
       "        0.27692308],\n",
       "       [0.12389381, 0.13013699, 0.47      , ..., 0.51181102, 0.19760479,\n",
       "        0.4       ]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "scaled_sk = scaler.fit_transform(inputs)\n",
    "scaled_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24778761, 0.39726027, 0.        , ..., 0.60629921, 0.13772455,\n",
       "        0.15384615],\n",
       "       [0.28318584, 0.52054795, 0.        , ..., 0.36220472, 0.20958084,\n",
       "        0.21538462],\n",
       "       [0.28318584, 0.43835616, 0.04      , ..., 0.40944882, 0.19161677,\n",
       "        0.21538462],\n",
       "       ...,\n",
       "       [0.15044248, 0.26712329, 0.13      , ..., 0.53543307, 0.25149701,\n",
       "        0.4       ],\n",
       "       [0.11504425, 0.35958904, 0.12      , ..., 0.65354331, 0.22754491,\n",
       "        0.27692308],\n",
       "       [0.12389381, 0.13013699, 0.47      , ..., 0.51181102, 0.19760479,\n",
       "        0.4       ]])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n"
     ]
    }
   ],
   "source": [
    "examples = min(counts)\n",
    "examples\n",
    "\n",
    "bad_indices = np.random.choice(np.where(classes == 'Bad')[0], examples)\n",
    "good_indices = np.random.choice(np.where(classes == 'Good')[0], examples)\n",
    "mid_indices = np.random.choice(np.where(classes == 'Mid')[0], examples)\n",
    "\n",
    "under_sampled_examples = scaled[bad_indices]\n",
    "under_sampled_classes = classes[bad_indices]\n",
    "\n",
    "under_sampled_examples = np.append(under_sampled_examples,scaled[good_indices], axis=0)\n",
    "under_sampled_classes = np.append(under_sampled_classes, classes[good_indices], axis=0)\n",
    "\n",
    "under_sampled_examples = np.append(under_sampled_examples,scaled[mid_indices], axis=0)\n",
    "under_sampled_classes = np.append(under_sampled_classes, classes[mid_indices], axis=0)\n",
    "\n",
    "len(under_sampled_examples)\n",
    "print(len(under_sampled_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n classes: 3\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(classes))\n",
    "print(\"n classes:\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 500. Total Error = 0.22749142563479316'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(1)\n",
    "N = len(inputs[0])\n",
    "mlp = MLP.MLPClassifier(Layer(6, N), Layer(n_classes, 6))\n",
    "\n",
    "mlp.train(under_sampled_examples, under_sampled_classes, eta=0.3, alpha=0.1, tol=1e-4, epoch_max=500, \n",
    "          print_status=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8518518518518519"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mlp.predict(under_sampled_examples)\n",
    "accuracy_score(under_sampled_classes, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[52,  0, 11],\n",
       "       [ 0, 57,  6],\n",
       "       [ 2,  9, 52]])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(under_sampled_classes, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='sgd', alpha=1e-5, activation='logistic', learning_rate='constant', tol=1e-10,\n",
    "                    learning_rate_init=0.5, momentum=0.5, hidden_layer_sizes=(N), random_state=1,\n",
    "                    shuffle=False, max_iter=6000)\n",
    "\n",
    "\n",
    "clf.fit(under_sampled_examples, mapped_under_sampled_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(under_sampled_examples)\n",
    "accuracy_score(np.array(mapped_under_sampled_classes), predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', alpha=1e-5, activation='logistic', learning_rate='constant', tol=1e-10,\n",
    "                    learning_rate_init=0.5, momentum=0.5, hidden_layer_sizes=(N), random_state=1,\n",
    "                    shuffle=False, max_iter=1000)\n",
    "\n",
    "\n",
    "clf.fit(normalized, mapped_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_classes[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.array(mapped_classes), predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 'default_features_1059_tracks.txt' file, the first 68 columns are audio features of the track, and the last two columns are the origin of the music, represented by latitude and longitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.161286</td>\n",
       "      <td>7.835325</td>\n",
       "      <td>2.911583</td>\n",
       "      <td>0.984049</td>\n",
       "      <td>-1.499546</td>\n",
       "      <td>-2.094097</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>-1.205671</td>\n",
       "      <td>1.849122</td>\n",
       "      <td>-0.425598</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.504263</td>\n",
       "      <td>0.351267</td>\n",
       "      <td>-1.018726</td>\n",
       "      <td>-0.174878</td>\n",
       "      <td>-1.089543</td>\n",
       "      <td>-0.668840</td>\n",
       "      <td>-0.914772</td>\n",
       "      <td>-0.836250</td>\n",
       "      <td>-15.75</td>\n",
       "      <td>-47.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.225763</td>\n",
       "      <td>-0.094169</td>\n",
       "      <td>-0.603646</td>\n",
       "      <td>0.497745</td>\n",
       "      <td>0.874036</td>\n",
       "      <td>0.290280</td>\n",
       "      <td>-0.077659</td>\n",
       "      <td>-0.887385</td>\n",
       "      <td>0.432062</td>\n",
       "      <td>-0.093963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.495712</td>\n",
       "      <td>-0.465077</td>\n",
       "      <td>-0.157861</td>\n",
       "      <td>-0.157189</td>\n",
       "      <td>0.380951</td>\n",
       "      <td>1.088478</td>\n",
       "      <td>-0.123595</td>\n",
       "      <td>1.391141</td>\n",
       "      <td>14.91</td>\n",
       "      <td>-23.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.692525</td>\n",
       "      <td>-0.517801</td>\n",
       "      <td>-0.788035</td>\n",
       "      <td>1.214351</td>\n",
       "      <td>-0.907214</td>\n",
       "      <td>0.880213</td>\n",
       "      <td>0.406899</td>\n",
       "      <td>-0.694895</td>\n",
       "      <td>-0.901869</td>\n",
       "      <td>-1.701574</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.637167</td>\n",
       "      <td>0.147260</td>\n",
       "      <td>0.217914</td>\n",
       "      <td>2.718442</td>\n",
       "      <td>0.972919</td>\n",
       "      <td>2.081069</td>\n",
       "      <td>1.375763</td>\n",
       "      <td>1.063847</td>\n",
       "      <td>12.65</td>\n",
       "      <td>-8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.735562</td>\n",
       "      <td>-0.684055</td>\n",
       "      <td>2.058215</td>\n",
       "      <td>0.716328</td>\n",
       "      <td>-0.011393</td>\n",
       "      <td>0.805396</td>\n",
       "      <td>1.497982</td>\n",
       "      <td>0.114752</td>\n",
       "      <td>0.692847</td>\n",
       "      <td>0.052377</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178325</td>\n",
       "      <td>-0.065059</td>\n",
       "      <td>-0.724247</td>\n",
       "      <td>-1.020687</td>\n",
       "      <td>-0.751380</td>\n",
       "      <td>-0.385005</td>\n",
       "      <td>-0.012326</td>\n",
       "      <td>-0.392197</td>\n",
       "      <td>9.03</td>\n",
       "      <td>38.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.570272</td>\n",
       "      <td>0.273157</td>\n",
       "      <td>-0.279214</td>\n",
       "      <td>0.083456</td>\n",
       "      <td>1.049331</td>\n",
       "      <td>-0.869295</td>\n",
       "      <td>-0.265858</td>\n",
       "      <td>-0.401676</td>\n",
       "      <td>-0.872639</td>\n",
       "      <td>1.147483</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.919463</td>\n",
       "      <td>-0.667912</td>\n",
       "      <td>-0.820172</td>\n",
       "      <td>-0.190488</td>\n",
       "      <td>0.306974</td>\n",
       "      <td>0.119658</td>\n",
       "      <td>0.271838</td>\n",
       "      <td>1.289783</td>\n",
       "      <td>34.03</td>\n",
       "      <td>-6.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  7.161286  7.835325  2.911583  0.984049 -1.499546 -2.094097  0.576000   \n",
       "1  0.225763 -0.094169 -0.603646  0.497745  0.874036  0.290280 -0.077659   \n",
       "2 -0.692525 -0.517801 -0.788035  1.214351 -0.907214  0.880213  0.406899   \n",
       "3 -0.735562 -0.684055  2.058215  0.716328 -0.011393  0.805396  1.497982   \n",
       "4  0.570272  0.273157 -0.279214  0.083456  1.049331 -0.869295 -0.265858   \n",
       "\n",
       "         7         8         9   ...          60        61        62  \\\n",
       "0 -1.205671  1.849122 -0.425598  ...   -1.504263  0.351267 -1.018726   \n",
       "1 -0.887385  0.432062 -0.093963  ...   -0.495712 -0.465077 -0.157861   \n",
       "2 -0.694895 -0.901869 -1.701574  ...   -0.637167  0.147260  0.217914   \n",
       "3  0.114752  0.692847  0.052377  ...   -0.178325 -0.065059 -0.724247   \n",
       "4 -0.401676 -0.872639  1.147483  ...   -0.919463 -0.667912 -0.820172   \n",
       "\n",
       "         63        64        65        66        67     68     69  \n",
       "0 -0.174878 -1.089543 -0.668840 -0.914772 -0.836250 -15.75 -47.95  \n",
       "1 -0.157189  0.380951  1.088478 -0.123595  1.391141  14.91 -23.51  \n",
       "2  2.718442  0.972919  2.081069  1.375763  1.063847  12.65  -8.00  \n",
       "3 -1.020687 -0.751380 -0.385005 -0.012326 -0.392197   9.03  38.74  \n",
       "4 -0.190488  0.306974  0.119658  0.271838  1.289783  34.03  -6.85  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('default_features_1059_tracks.txt', header=None)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values\n",
    "#n_data = data_normalization(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79230368, 0.76352811, 0.55478876, ..., 0.06751881, 0.2172705 ,\n",
       "        0.17155709],\n",
       "       [0.16001751, 0.11334916, 0.07269012, ..., 0.37235778, 0.55801289,\n",
       "        0.27429797],\n",
       "       [0.07630057, 0.07861345, 0.04740195, ..., 0.32756458, 0.5328962 ,\n",
       "        0.33949891],\n",
       "       ...,\n",
       "       [0.0690223 , 0.06608503, 0.04021757, ..., 0.1034345 , 0.8516337 ,\n",
       "        0.45636455],\n",
       "       [0.04854589, 0.03092565, 0.6375824 , ..., 0.0680776 , 1.        ,\n",
       "        0.47952749],\n",
       "       [0.12567753, 0.11333572, 0.07745744, ..., 0.38213486, 1.        ,\n",
       "        0.47952749]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "scaled = scaler.fit_transform(data)\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = scaled[:,:-2]\n",
    "outputs = scaled[:,-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79230368, 0.76352811, 0.55478876, ..., 0.07543846, 0.04642531,\n",
       "        0.06751881],\n",
       "       [0.16001751, 0.11334916, 0.07269012, ..., 0.30406071, 0.16613812,\n",
       "        0.37235778],\n",
       "       [0.07630057, 0.07861345, 0.04740195, ..., 0.43319408, 0.39300562,\n",
       "        0.32756458],\n",
       "       ...,\n",
       "       [0.0690223 , 0.06608503, 0.04021757, ..., 0.06902011, 0.03600491,\n",
       "        0.1034345 ],\n",
       "       [0.04854589, 0.03092565, 0.6375824 , ..., 0.14456785, 0.06666088,\n",
       "        0.0680776 ],\n",
       "       [0.12567753, 0.11333572, 0.07745744, ..., 0.84472242, 0.56410745,\n",
       "        0.38213486]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2172705 , 0.17155709],\n",
       "       [0.55801289, 0.27429797],\n",
       "       [0.5328962 , 0.33949891],\n",
       "       ...,\n",
       "       [0.8516337 , 0.45636455],\n",
       "       [1.        , 0.47952749],\n",
       "       [1.        , 0.47952749]])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(inputs[0])\n",
    "mlpR = MLP.MLPRegressor(Layer(int(math.floor(n_features/2)), n_features), Layer(2, int(math.floor(n_features/2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 2000. Total Error = 0.0029200341639355025'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlpR.train(inputs, outputs, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=2000, print_status=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlpR.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_predicted):\n",
    "    y_true = np.array(y_true)\n",
    "    y_predicted = np.array(y_predicted)\n",
    "    \n",
    "    # Average Mean Squared error per sample\n",
    "    samples_error = np.average((y_true-y_predicted)**2, axis=1) \n",
    "    \n",
    "    # Average Mean Squared Error for all samples\n",
    "    return np.average(samples_error, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(outputs, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(outputs, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
