{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.utils import shuffle as shuffle_data\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# Layer represents a MLP Layer\n",
    "# It has two main properties:\n",
    "#      - a weigth matrix containing the weights of the layer's neurons. Each line represents a neuron and \n",
    "#        the columns represent its corresponding weights\n",
    "#      - a bias vector, containing the neurons's bias\n",
    "# Since during the backpropagation we need to compute the weights variation using the old ones, the \n",
    "# updated_weights and updated_bias properties store the new values until the update method is called\n",
    "class Layer:\n",
    "    # Create a new Layer with 'size' neurons, each one linked to 'inputs_size' inputs\n",
    "    def __init__(self, size, inputs_size):\n",
    "        self.size = size\n",
    "        self.inputs_size = inputs_size\n",
    "        self.weights = np.array([[random.uniform(-0.1, 0.1) for j in range(inputs_size)] for i in range(size)])\n",
    "        self.bias = np.array([random.uniform(-0.1,0.1) for i in range(size)])\n",
    "        \n",
    "        self.d_weights_current = np.zeros((size, inputs_size))\n",
    "        self.d_bias_current = np.zeros(size)\n",
    "        self.d_weights_old = np.zeros((size, inputs_size))\n",
    "        self.d_bias_old = np.zeros(size)\n",
    "    \n",
    "    # update updates the weights and bias matrices with the values stored in the updated ones\n",
    "    def update(self, eta, alpha):\n",
    "        #self.weights = np.copy(self.updated_weights)\n",
    "        #self.bias = np.copy(self.updated_bias)\n",
    "        \n",
    "        self.weights = self.weights + eta*self.d_weights_current + alpha*self.d_weights_old \n",
    "        self.bias = self.bias + eta*self.d_bias_current + alpha*self.d_bias_old\n",
    "        \n",
    "        self.d_weights_old = self.d_weights_current\n",
    "        self.d_bias_old = self.d_bias_current\n",
    "        \n",
    "    # description prints a layer description\n",
    "    def description(self):\n",
    "        print(\"Layer Info\")\n",
    "        print(\"Weights: \\n\", self.weights)\n",
    "        print(\"Bias: \\n \", self.bias)\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0/(1.0+ math.exp(-x))\n",
    "\n",
    "logistic_vec = np.vectorize(logistic)\n",
    "\n",
    "def logistic_derivate(x):\n",
    "    return x*(1.0-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    # MLP creation. One might pass the MLP layers as parameters or add them later using the add_layer method.\n",
    "    # The classification parameter defines if the MLP will be used for a classification or regression problem\n",
    "    def __init__(self, *layers, classifier=True):\n",
    "        self.classifier = classifier\n",
    "        if classifier:\n",
    "            # Map each class label to a vector with a single 1\n",
    "            # Ex: Class 0 -> [1,0]\n",
    "            #     Class 1 -> [0,1]\n",
    "            self.class_mapping = dict()  \n",
    "            # Unmap each class vector to the corresponding class label\n",
    "            # Ex: [1,0] -> Class 0 \n",
    "            #     [0,1] -> Class 1\n",
    "            self.class_unmapping = dict()\n",
    "            \n",
    "        self.layers = list()\n",
    "        for layer in layers:\n",
    "            self.add_layer(layer)\n",
    "    \n",
    "    # Shortcut to create a classifier MLP\n",
    "    @classmethod\n",
    "    def MLPClassifier(cls, *layers):\n",
    "        return cls(classifier=True, *layers)   \n",
    "    \n",
    "    # Shortcut to create a regressor MLP\n",
    "    @classmethod\n",
    "    def MLPRegressor(cls, *layers):\n",
    "        return cls(classifier=False, *layers)\n",
    "    \n",
    "    # add_layer adds a new layer on the MLP. It verifies whether or not the new layer is compatible with the MLP\n",
    "    def add_layer(self, layer):\n",
    "        # If there's already a layer in the MLP, verify if the new layer is compatible\n",
    "        if len(self.layers) > 0:\n",
    "            if layer.inputs_size != self.layers[-1].size:\n",
    "                print(\"The new layer is incompatible with the MLP\")\n",
    "                print(\"Please, use a layer where each neuron has the same amount of inputs as the number\" \\\n",
    "                     \"of neurons in the MLP last layer\")\n",
    "        \n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    # description prints the info about the MLP layers\n",
    "    def description(self):\n",
    "        print(\"MLP Classifier?: \", self.classifier)\n",
    "        print(\"-------------------------\")\n",
    "        print(\"MLP Info:\")\n",
    "        for layer, i in zip(self.layers, range(len(self.layers))):\n",
    "            print(\"--- Layer: %d ---\" % i)\n",
    "            layer.description()\n",
    "            \n",
    "    # __get_class_mapping gets the class labels in the classes list and builds the mapping dicionaries\n",
    "    # class_mapping and class_unmapping\n",
    "    def __get_class_mapping(self, classes):\n",
    "        class_labels = np.unique(classes)\n",
    "        \n",
    "        for c in range(len(class_labels)):\n",
    "            class_label = class_labels[c]\n",
    "            class_vector = np.zeros(len(class_labels))\n",
    "            class_vector[c] = 1\n",
    "    \n",
    "            self.class_mapping[class_label] = class_vector\n",
    "            \n",
    "            # We can't use a list as a hash key. So transform it into a tuple\n",
    "            self.class_unmapping[tuple(class_vector)] = class_label\n",
    "        \n",
    "    # __convert_class_labels_to_vectors converts a list with class labels to a list with \n",
    "    # vectors that maps each class label\n",
    "    def __convert_class_labels_to_vectors(self, class_labels):\n",
    "        return [self.class_mapping[c] for c in class_labels]\n",
    "    \n",
    "    # __convert_class_vectors_to_labels converts a list with class vectors to a list with \n",
    "    # the corresponding class labels\n",
    "    def __convert_class_vectors_to_labels(self, class_vectors):\n",
    "        return [self.class_unmapping[tuple(class_vector)] for class_vector in class_vectors]\n",
    "        \n",
    "        \n",
    "    # fast_forward computes the ouput for a given input vector\n",
    "    def fast_forward(self,input_v):\n",
    "        # We need to store each layer input in order to perform the backpropagation\n",
    "        self.inputs = list()\n",
    "    \n",
    "        # The input is applied in a layer weights matrix and the bias is added in the result\n",
    "        # Then, the logistic function is applied to each layer neuron result\n",
    "        # For a layer, we have a final output vector where each component i represents the output\n",
    "        # of the neuron i\n",
    "        for layer in self.layers:\n",
    "            self.inputs.append(input_v)\n",
    "            output = logistic_vec(layer.weights @ input_v + layer.bias)\n",
    "            \n",
    "            # The output of the current layer is the input of the next one\n",
    "            input_v = output\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    # train trains the MLP using the examples passed in the samples parameter\n",
    "    # The expected output for each example must be passed in the classes parameter;\n",
    "    # eta represents the MLP learning rate;\n",
    "    # tol represents the error tolerance. The MLP is trained until the cumulative squared error for all example\n",
    "    #     is less than the tol value\n",
    "    # print_status prints the output for each example during the training phase\n",
    "    def train(self, samples, classes, eta=0.5, alpha=0, tol=1e-2, epoch_max=2000, \n",
    "              print_status=False, shuffle=True):\n",
    "        # Map the class labels to output vectors if it's a classification problem\n",
    "        if self.classifier:\n",
    "            self.__get_class_mapping(classes)\n",
    "            classes = self.__convert_class_labels_to_vectors(classes)\n",
    "                \n",
    "        error = tol\n",
    "        new_error = 3*tol\n",
    "        epoch = 0\n",
    "        \n",
    "        # The training stops when the max number of epochs is reached or the Kramer and Sangiovanni-Vicentelly\n",
    "        # criteria is valid. According to it, we can consider that the BP converged when the average mean squared\n",
    "        # error is less than a given tolerance\n",
    "        while (abs(new_error - error) > tol and epoch < epoch_max):\n",
    "            epoch += 1\n",
    "            error = 0\n",
    "            new_error = 0\n",
    "            \n",
    "            # Suffles samples to avoid saturation if training with samples beloging to the same class\n",
    "            # one after another\n",
    "            if shuffle:\n",
    "                samples, classes = shuffle_data(samples, classes)\n",
    "            \n",
    "            for input_v, t in zip(samples, classes):  \n",
    "                # ---- Compute the output for the given input vector ----\n",
    "                output = self.fast_forward(input_v)\n",
    "                \n",
    "                # Compute the mean squared error before the backpropagation\n",
    "                error_sample = pow((np.array(t)-np.array(output)),2)\n",
    "                # We need to sum the error of each component when the output is a vector\n",
    "                error += sum(error_sample)/len(samples)\n",
    "                \n",
    "                if (print_status == True):\n",
    "                    print(\"\\ttraining example: %s from class %s\" % (input_v, t), end = \" \")\n",
    "                    print(\"y = \", output)\n",
    "     \n",
    "                # ---- Backpropagation ----\n",
    "                # Compute the new weights of each layer\n",
    "                # Remark: the udpated weights are stored as a layer property and the layer is updated once \n",
    "                # the backpropagation is finished\n",
    "                # It's necessary to do so in order to compute the delta value for the inner layers. We need \n",
    "                # to use the weights that caused the error to compute the delta instead of the updated weights\n",
    "                for l in reversed(range(len(self.layers))): # Traverse the layers in reversed order\n",
    "                    layer = self.layers[l]\n",
    "             \n",
    "                    deltas = list()\n",
    "                    # Compute the delta for each layer neuron n\n",
    "                    for n in range(len(layer.weights)):\n",
    "                        # Last Layer\n",
    "                        if l == (len(self.layers)-1):\n",
    "                            delta = (t[n]-output[n])*logistic_derivate(output[n])\n",
    "                            \n",
    "                        # Inner Layer\n",
    "                        else:\n",
    "                            # output of the current layer is the input of the next one\n",
    "                            neuron_output = self.inputs[l+1][n]\n",
    "                            # weights of each neuron output\n",
    "                            errors_weights = self.layers[l+1].weights[:,n]\n",
    "                            \n",
    "                            delta = np.dot(delta_next_layer,errors_weights)*logistic_derivate(neuron_output)\n",
    "                              \n",
    "                        # Computes the weights and bias variation for the neuron n\n",
    "                        for w in range(len(layer.weights[n])):\n",
    "                            layer.d_weights_current[n][w] = delta*self.inputs[l][w]\n",
    "                        layer.d_bias_current[n] = delta*1 # bias input = 1\n",
    "                        \n",
    "                        #for w in range(len(layer.weights[n])):\n",
    "                        #    layer.updated_weights[n][w] = layer.weights[n][w] + eta*delta*self.inputs[l][w]\n",
    "                        #layer.updated_bias[n] = layer.bias[n] + (eta*delta*1) # bias input = 1\n",
    "\n",
    "                        # Store the neuron delta\n",
    "                        deltas.append(delta)\n",
    "                    \n",
    "                    # The neurons' delta of the current layer will be used to compute the deltas of the \n",
    "                    # next inner layer\n",
    "                    delta_next_layer = np.array(deltas)\n",
    "                     \n",
    "                # Once the backpropagation is finished for the current example, update all the weigths and bias\n",
    "                for layer in self.layers:\n",
    "                    layer.update(eta, alpha)\n",
    "                    \n",
    "                # Compute the new error mean squared error\n",
    "                output = self.fast_forward(input_v)\n",
    "                error_sample = pow((np.array(t)-np.array(output)),2)\n",
    "                #print(\"error sample: \",error_sample)\n",
    "                new_error += sum(error_sample)/len(samples)\n",
    "            \n",
    "            # End of a epoch\n",
    "            if epoch%1 == 0: # Print status only after each 100 iterations \n",
    "                clear_output(wait=True)\n",
    "                display(\"End of epoch \" + str(epoch) + \". Total Error = \" + str(new_error))\n",
    "        \n",
    "        # End of training         \n",
    "        clear_output(wait=True)\n",
    "        display(\"End of epoch \" + str(epoch) + \". Total Error = \" + str(new_error))\n",
    "        \n",
    "    # predicts gets a list of input samples and returns a list with the predicted outputs\n",
    "    def predict(self, samples):\n",
    "        outputs = list()\n",
    "        for input_v in samples:\n",
    "            probs = self.fast_forward(input_v)\n",
    "            \n",
    "            if self.classifier:\n",
    "                class_pos = np.argmax(probs)\n",
    "                output = np.zeros(len(probs))\n",
    "                output[class_pos] = 1\n",
    "            \n",
    "                #outputs.append(self.class_unmapping[tuple(output)])\n",
    "                outputs.append(output)\n",
    "                \n",
    "            else:\n",
    "                outputs.append(probs)\n",
    "    \n",
    "        if self.classifier:\n",
    "            return self.__convert_class_vectors_to_labels(outputs)\n",
    "        \n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data transforms data in order to all points have mean 0 and variance 1\n",
    "def normalize_data(data):\n",
    "    normalized_columns = list()\n",
    "    for c in range(len(data[0])):\n",
    "        col = data[:,c]\n",
    "        normalized_columns.append((col - np.mean(col))/np.std(col))\n",
    "\n",
    "    return np.array(normalized_columns).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_data transforms data in order to all points be in the interval [0,1]\n",
    "def scale_data(data):\n",
    "    normalized_columns = list()\n",
    "    for c in range(len(data[0])):\n",
    "        col = data[:,c]\n",
    "        normalized_columns.append((col-np.min(col))/(np.max(col)-np.min(col)))\n",
    "\n",
    "    return np.array(normalized_columns).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate(real_classes, predicted_classes, display=True):\n",
    "    acc = accuracy_score(real_classes, predicted_classes)\n",
    "    \n",
    "    class_labels = np.sort(np.unique(real_classes))\n",
    "    cm = confusion_matrix(real_classes, predicted_classes, labels=class_labels)\n",
    "    df = pd.DataFrame(cm)\n",
    "    df.columns = class_labels\n",
    "    df.index = class_labels\n",
    "    \n",
    "    # Acurracy per class\n",
    "    accs = list()\n",
    "    for c in range(len(cm)):\n",
    "        accs.append(cm[c,c]/sum(cm[c,:]))\n",
    "    df[\"Accuracy\"] = accs\n",
    "    \n",
    "    avg_acc = np.average(accs)\n",
    "    \n",
    "    if display == True:\n",
    "        print(\"Accuracy: %.2f%%\" % (acc*100))\n",
    "        \n",
    "        print(\"Confusion Matrix and Accuracy per class:\")\n",
    "        print(df)\n",
    "\n",
    "        print(\"Average accuracy per class: %.2f%%\" % (avg_acc*100))\n",
    "        \n",
    "    return acc, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o problema de classificação, vamos pegar o conjunto de dados fornecidos e vamos normalizá-los de forma que cada valor de feature esteja no intervalo (0,1). Isso será feito para evitar a saturação da saída dos neurônios e melhorar a convergência do algoritmo de aprendizagem.\n",
    "\n",
    "A seguir, tomaremos um conjunto de treinamento consistindo em 70% da base original. Iremos avaliar o impacto da arquitetura da rede assim como da variação de parâmetros de aprendizado no valor de acurácia obtido na classificação do conjunto de teste. \n",
    "\n",
    "Iremos calcular a acurácia total e a acurácia por classe. Uma vez que não estamos impondo penalidades diferentes para erros cometidos em determinadas classes, vamos considerar como melhor arquitetura aquela que fornece maior valor para a acurácia total, embora isso possa não refletir em uma acurácia por classe elevada! Mais tarde, iremos tratar melhor o balanceamento entre as classes a fim de que o aprendizado da rede seja adequado para todas elas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "0           0            7.4              0.70         0.00             1.9   \n",
       "1           1            7.8              0.88         0.00             2.6   \n",
       "2           2            7.8              0.76         0.04             2.3   \n",
       "3           3           11.2              0.28         0.56             1.9   \n",
       "4           4            7.4              0.70         0.00             1.9   \n",
       "\n",
       "   chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
       "0      0.076                 11.0                  34.0   0.9978  3.51   \n",
       "1      0.098                 25.0                  67.0   0.9968  3.20   \n",
       "2      0.092                 15.0                  54.0   0.9970  3.26   \n",
       "3      0.075                 17.0                  60.0   0.9980  3.16   \n",
       "4      0.076                 11.0                  34.0   0.9978  3.51   \n",
       "\n",
       "   sulphates  alcohol category  \n",
       "0       0.56      9.4      Mid  \n",
       "1       0.68      9.8      Mid  \n",
       "2       0.65      9.8      Mid  \n",
       "3       0.58      9.8      Mid  \n",
       "4       0.56      9.4      Mid  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('winequality-red.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.4     0.7     0.      1.9     0.076  11.     34.      0.9978  3.51\n",
      "   0.56    9.4   ]\n",
      " [ 7.8     0.88    0.      2.6     0.098  25.     67.      0.9968  3.2\n",
      "   0.68    9.8   ]]\n",
      "['Mid' 'Mid' 'Mid' ... 'Mid' 'Mid' 'Mid']\n"
     ]
    }
   ],
   "source": [
    "# data separation\n",
    "inputs = df[df.columns[1:-1]].values\n",
    "classes = df[df.columns[-1]].values\n",
    "\n",
    "print(inputs[0:2,:])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bad' 'Good' 'Mid']\n",
      "[  63  217 1319]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(classes, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = scale_data(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split - Training - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, classes, test_size=0.3, \n",
    "                                                    stratify=classes,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different network architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 layer - 5 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "N1 = 5\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(n_classes, N1))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Layer - 11 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 11\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(n_classes, N1))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Layer - 22 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 22\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(n_classes, N1))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 5 Neurons - 5 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 5\n",
    "N2 = 5\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 5 Neurons - 11 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 5\n",
    "N2 = 11\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 5 Neurons - 22 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 5\n",
    "N2 = 22\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 11Neurons - 5 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 11\n",
    "N2 = 5\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 11Neurons - 11 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 11\n",
    "N2 = 11\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 11Neurons - 22 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 11\n",
    "N2 = 22\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 22Neurons - 5 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 22\n",
    "N2 = 5\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 22Neurons - 11 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 22\n",
    "N2 = 11\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 22Neurons - 22 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 22\n",
    "N2 = 22\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of epochs used during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos analisar como o número de epochs usadas durante o treinamento interfere na acurácia obtida no conjunto de testes! Para isso, vamos utilizar a arquitetura de rede que obteve melhor desempenho:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = dict()\n",
    "best_N1 = 11\n",
    "best_N2 = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[epochs] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 400 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 400\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[epochs] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 800 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 800\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[epochs] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[epochs] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[epochs] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_accs = pd.DataFrame(accs)\n",
    "epochs_accs.index = [\"Total accuracy\", \"Accuracy per class\"]\n",
    "print(epochs_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repare que embora a Loss Function decresça para 1000 e 2000 epochs, ao avaliar o modelo no conjunto de testes, há uma queda na acurácia! isso ilustra a ocorrência de um overfitting do modelo. Portanto, nas próximas etapas, iremos tomar 800 epochs como o máximo de iterações durante a fase de treinamento, uma vez que ela mantém um bom nível de acurácia total, equanto eleva a acurácia por classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epochs = 800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate and Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixando o número de epochs em 500 e utilizando a arquitura de rede com melhor desempenho (2 camadas intermediárias: 5 neurônios na primeira e 11 na segunda), vamos variar os parâmetros learning rate (eta) e momentum (alfa), e vamos observar como eles interferem no aprendizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 0.3 e Momentum = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.3\n",
    "alpha = 0.3\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 0.3 e Momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.3\n",
    "alpha = 0.5\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 0.3 e Momentum = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.3\n",
    "alpha = 0.8\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 0.5 e Momentum = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.5\n",
    "alpha = 0.3\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 0.5 e Momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.5\n",
    "alpha = 0.5\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 0.5 e Momentum = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.5\n",
    "alpha = 0.8\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 0.8 e Momentum = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.8\n",
    "alpha = 0.3\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 0.8 e Momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.8\n",
    "alpha = 0.5\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 0.8 e Momentum = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.8\n",
    "alpha = 0.8\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 1 e Momentum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1\n",
    "alpha = 1\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = store\n",
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in accs.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    \n",
    "#store = accs\n",
    "#store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc = pd.DataFrame()\n",
    "avg_acc = pd.DataFrame()\n",
    "\n",
    "for run, acc in accs.items():\n",
    "    total_acc.at[str(run[0]), str(run[1])] = acc[0]\n",
    "    avg_acc.at[str(run[0]), str(run[1])] = acc[1]\n",
    "    \n",
    "print(total_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que, quando o learning rate e momentum são de magnitude elevada (próximos à 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eta   = 0.5\n",
    "best_alpha = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> Agora, vamos variar o tamanho dos conjuntos de treinamento e teste utilizando a melhor arquitetura encontrada acima e os melhores valores de learning rate e momentum. Utilizaremos incialmente 70% dos dados para treinamento, aumentando gradativamente esse valor até 90%.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70% for training, 30% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, classes, test_size=0.3, \n",
    "                                                    stratify=classes,random_state=42)\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 75% for training, 25% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, classes, test_size=0.25, \n",
    "                                                    stratify=classes,random_state=42)\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80% for training, 20% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, classes, test_size=0.2, \n",
    "                                                    stratify=classes,random_state=42)\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 85% for training, 15% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, classes, test_size=0.15, \n",
    "                                                    stratify=classes,random_state=42)\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 90% for training, 10% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, classes, test_size=0.1, \n",
    "                                                    stratify=classes,random_state=42)\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95% for training, 5% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, classes, test_size=0.05, \n",
    "                                                    stratify=classes,random_state=42)\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir das classificações realizadas, é possível observar que a variação nos tamanhos dos conjuntos de treinamento e teste resultaram em mudanças pouco significantes nos valores de acurácia obtidos. \n",
    "\n",
    "Vale destacar que utilizando 70% dos dados para treinamento e 30% dos dados para teste, obtivemos a melhor acurácia média por classe, enquanto que utilizando 85% dos dados para treinamento e 15% para teste, foi obtida a melhor acurácia geral.\n",
    "\n",
    "Assim, como a diferença na acurácia geral de ambos os casos citados é de pouco mais de 1%, a divisão entre 70% e 30% será considerada como a que apresentou melhores resultados e será utilizada para os próximos testes daqui em diante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_size = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Pre Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar nas classificações acima, na base de dados considerada, há poucos exemplos da classe Bad. Como consequência, como estamos dando o mesmo peso para um erro cometido em qualquer classe, a rede acaba por classificar a maioria dos exemplos como sendo pertecentes à classe com maior número de exemplos ('Mid').\n",
    "\n",
    "A seguir, vamos considerar uma base de dados com um número balanceado entre as classes. Para tal, vamos tomar o tamanho da menor classe e escolher exemplos aleatórios das demais classes para igualar esse número.\n",
    "\n",
    "Também vamos considerar uma base de dados com exemplos artificiais que serão criados para igualar o número de exemplos da menor classe com o número de exemplos da classe intermediária. Não igualaremos o número de exemplares pela classe de maior cardinalidade, uma vez que, no mundo real, é normal que haja mais itens de qualidade intermediária que itens de qualidade ruim ou boa. \n",
    "\n",
    "Para melhor comparar as base de dados e não ser influenciado pela aleatoriedade com a qual os conjuntos de treinamento e teste são escolhidos, iremos avaliar os modelos utilizando CrossValidation Stratified com 10 folds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the minor quantity of examples, among all the classes from the dataset\n",
    "examples = min(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad'\n",
      " 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad'\n",
      " 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad'\n",
      " 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad'\n",
      " 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad' 'Bad'\n",
      " 'Bad' 'Bad' 'Bad']\n"
     ]
    }
   ],
   "source": [
    "# randomly chooses 63 indexes of examples from each class \n",
    "bad_indices = np.random.choice(np.where(classes == 'Bad')[0], examples)\n",
    "good_indices = np.random.choice(np.where(classes == 'Good')[0], examples)\n",
    "mid_indices = np.random.choice(np.where(classes == 'Mid')[0], examples)\n",
    "\n",
    "# stores the chosen examples from 'Bad' class, as well as the same amount of labels from it\n",
    "under_sampled_examples = scaled_inputs[bad_indices]\n",
    "under_sampled_classes = classes[bad_indices]\n",
    "\n",
    "print(under_sampled_classes)\n",
    "\n",
    "# stores the chosen examples from 'Good' class, as well as the same amount of labels from it\n",
    "under_sampled_examples = np.append(under_sampled_examples,scaled_inputs[good_indices], axis=0)\n",
    "under_sampled_classes = np.append(under_sampled_classes, classes[good_indices], axis=0)\n",
    "\n",
    "# stores the chosen examples from 'Mid' class, as well as the same amount of labels from it \n",
    "under_sampled_examples = np.append(under_sampled_examples,scaled_inputs[mid_indices], axis=0)\n",
    "under_sampled_classes = np.append(under_sampled_classes, classes[mid_indices], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the new dataset in a training set and a test set, using 70% to 30% proportion\n",
    "X_train, X_test, y_train, y_test = train_test_split(under_sampled_examples, under_sampled_classes, \n",
    "                                                    test_size=best_test_size, \n",
    "                                                    stratify=under_sampled_classes,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Cross Validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the best architecture, number of epochs and alpha and eta values, all previously obtained,\n",
    "# for classifying the new examples, without using Stratified K-Fold metrics\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> Como é possível observar, a acurácia obtida na classificação utilizando undersampling piorou em cerca de 10% em comparação com a classificação realizada utilizando a base de dados original. Porém, a acurácia média por classes melhorou consideravelmente, em cerca de 20%, quando comparada à melhor acurácia por classe obtida anteriormente, de aproximadamente 53%. Isso se deve ao fato de que a quantidade de exemplos disponíveis para cada classe é a mesma, de forma que a classificação não se torna enviesada, favorecendo os exemplos da classe majoritária em detrimento das outras.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Cross Validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 800. Total Error = 0.04805420786560231'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 87.30%\n",
      "Average accuracy per class: 87.30%\n"
     ]
    }
   ],
   "source": [
    "# uses the best architecture, number of epochs and alpha and eta values, all previously obtained,\n",
    "# for classifying the new examples, using Stratified K-Fold metrics, for K = 10.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "accuracies = list()\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "\n",
    "X = under_sampled_examples\n",
    "y = under_sampled_classes\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(train_index, test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "              print_status=False, shuffle=True)\n",
    "\n",
    "    predicted  = mlp.predict(X_test)\n",
    "    acc = evaluate(y_test, predicted, display=False)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "mean_acc = np.array([])\n",
    "mean_classes_acc = np.array([])\n",
    "\n",
    "for i in range(len(accuracies)):\n",
    "    mean_acc = np.append(mean_acc, accuracies[i][0])\n",
    "    mean_classes_acc = np.append(mean_classes_acc, accuracies[i][1])\n",
    "\n",
    "print(\"Average accuracy: \" + '{:.2f}'.format(np.mean(mean_acc)*100) + \"%\")\n",
    "print(\"Average accuracy per class: \" + '{:.2f}'.format(np.mean(mean_classes_acc)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on the complete dataset, trained with the undersampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.12%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad    19     0    0  1.000000\n",
      "Good    2    47   16  0.723077\n",
      "Mid   112    95  189  0.477273\n",
      "Average accuracy per class: 73.34%\n"
     ]
    }
   ],
   "source": [
    "predicted  = mlp.predict(X_test)\n",
    "acc = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# specify the class targeted by the resampling. The number of samples in the different classes will be equalized\n",
    "# 'not majority': resample all classes but the majority class\n",
    "sm = SMOTE(ratio='not majority')\n",
    "\n",
    "df_majority = df[df[\"category\"] == \"Mid\"]\n",
    "df_inter = df[df[\"category\"] == \"Good\"]\n",
    "df_minority = df[df[\"category\"] == \"Bad\"]\n",
    "\n",
    "# new_df containing only intermediate and minority classes\n",
    "new_df = df_inter.append(df_minority)\n",
    "\n",
    "# resample the dataset, using parameters: matrix containing the data which have to be sampled and corresponding \n",
    "# label for each sample in matrix\n",
    "# 'over_sampled_dfX': The array containing the resampled data\n",
    "# 'over_sampled_dfY': The corresponding label of over_sampled_dfX\n",
    "over_sampled_dfX, over_sampled_dfY = sm.fit_sample(new_df.drop('category', axis=1), new_df['category'])\n",
    "\n",
    "# over_sampled_df containing the resampled data (intermediate and minority classes)\n",
    "over_sampled_df = pd.concat([pd.DataFrame(over_sampled_dfX), pd.DataFrame(over_sampled_dfY)], axis=1)\n",
    "over_sampled_df.columns = new_df.columns\n",
    "\n",
    "# append majority class\n",
    "over_sampled_df = over_sampled_df.append(df_majority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.300e+00 6.500e-01 0.000e+00 1.200e+00 6.500e-02 1.500e+01 2.100e+01\n",
      "  9.946e-01 3.390e+00 4.700e-01 1.000e+01]\n",
      " [7.800e+00 5.800e-01 2.000e-02 2.000e+00 7.300e-02 9.000e+00 1.800e+01\n",
      "  9.968e-01 3.360e+00 5.700e-01 9.500e+00]]\n",
      "['Good' 'Good' 'Good' ... 'Mid' 'Mid' 'Mid']\n"
     ]
    }
   ],
   "source": [
    "# Separação dos Dados\n",
    "over_sampled_inputs = over_sampled_df[over_sampled_df.columns[1:-1]].values\n",
    "over_sampled_classes = over_sampled_df[over_sampled_df.columns[-1]].values\n",
    "\n",
    "print(over_sampled_inputs[0:2,:])\n",
    "print(over_sampled_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23893805, 0.3630137 , 0.        , ..., 0.51181102, 0.08383234,\n",
       "        0.24615385],\n",
       "       [0.28318584, 0.31506849, 0.02      , ..., 0.48818898, 0.14371257,\n",
       "        0.16923077],\n",
       "       [0.34513274, 0.10958904, 0.56      , ..., 0.44094488, 0.25149701,\n",
       "        0.32307692],\n",
       "       ...,\n",
       "       [0.15044248, 0.26712329, 0.13      , ..., 0.53543307, 0.25149701,\n",
       "        0.4       ],\n",
       "       [0.11504425, 0.35958904, 0.12      , ..., 0.65354331, 0.22754491,\n",
       "        0.27692308],\n",
       "       [0.12389381, 0.13013699, 0.47      , ..., 0.51181102, 0.19760479,\n",
       "        0.4       ]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "over_sampled_scaled_inputs = scale_data(over_sampled_inputs)\n",
    "over_sampled_scaled_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bad' 'Good' 'Mid']\n",
      "[ 217  217 1319]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(over_sampled_classes, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the new dataset in a training set and a test set, using 70% to 30% proportion\n",
    "X_train, X_test, y_train, y_test = train_test_split(over_sampled_scaled_inputs, over_sampled_classes, \n",
    "                                                    test_size=best_test_size, \n",
    "                                                    stratify=over_sampled_classes,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Cross Validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 800. Total Error = 0.141255717398671'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.95%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad    27     1   37  0.415385\n",
      "Good    0    24   41  0.369231\n",
      "Mid    16    21  359  0.906566\n",
      "Average accuracy per class: 56.37%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.779467680608365, 0.5637270137270137)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uses the best architecture, number of epochs and alpha and eta values, all previously obtained,\n",
    "# for classifying the new examples, without using Stratified K-Fold metrics\n",
    "random.seed(0)\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Cross Validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 200. Total Error = 0.1484945660018888'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 84.83%\n",
      "Average accuracy per class: 84.83%\n"
     ]
    }
   ],
   "source": [
    "# uses the best architecture, number of epochs and alpha and eta values, all previously obtained,\n",
    "# for classifying the new examples, using Stratified K-Fold metrics, for K = 10.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "accuracies = list()\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "\n",
    "X = over_sampled_scaled_inputs\n",
    "y = over_sampled_classes\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(train_index, test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=200, \n",
    "              print_status=False, shuffle=True)\n",
    "\n",
    "    predicted  = mlp.predict(X_test)\n",
    "    acc = evaluate(y_test, predicted, display=False)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "mean_acc = np.array([])\n",
    "mean_classes_acc = np.array([])\n",
    "\n",
    "for i in range(len(accuracies)):\n",
    "    mean_acc = np.append(mean_acc, accuracies[i][0])\n",
    "    mean_classes_acc = np.append(mean_classes_acc, accuracies[i][1])\n",
    "\n",
    "print(\"Average accuracy: \" + '{:.2f}'.format(np.mean(mean_acc)*100) + \"%\")\n",
    "print(\"Average accuracy per class: \" + '{:.2f}'.format(np.mean(mean_classes_acc)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on the complete dataset, trained with the oversampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.42%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad    13     0    6  0.684211\n",
      "Good    1    54   10  0.830769\n",
      "Mid    48    29  319  0.805556\n",
      "Average accuracy per class: 77.35%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, classes, test_size=0.3, \n",
    "                                                    stratify=classes,random_state=42)\n",
    "\n",
    "predicted  = mlp.predict(X_test)\n",
    "acc = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui será feito o treinamento e teste utilizando o conjunto de dados completo, sem utilização de undersampling ou oversampling, com o método de validação cruzada Stratified K-Fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the best architecture, number of epochs and alpha and eta values, all previously obtained,\n",
    "# for classifying the new examples, using Stratified K-Fold metrics, for K = 10.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "accuracies = list()\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "\n",
    "X = scaled_inputs\n",
    "y = classes\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    mlp.train(X_train, y_train, eta=best_eta, alpha=best_alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "              print_status=False, shuffle=True)\n",
    "\n",
    "    predicted  = mlp.predict(X_test)\n",
    "    acc = evaluate(y_test, predicted, display=False)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "mean_acc = np.array([])\n",
    "mean_classes_acc = np.array([])\n",
    "\n",
    "for i in range(len(accuracies)):\n",
    "    mean_acc = np.append(mean_acc, accuracies[i][0])\n",
    "    mean_classes_acc = np.append(mean_classes_acc, accuracies[i][1])\n",
    "\n",
    "print(\"Average accuracy: \" + '{:.2f}'.format(np.mean(mean_acc)*100) + \"%\")\n",
    "print(\"Average accuracy per class: \" + '{:.2f}'.format(np.mean(mean_classes_acc)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
