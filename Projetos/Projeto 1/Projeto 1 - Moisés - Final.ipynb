{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.utils import shuffle as shuffle_data\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# Layer represents a MLP Layer\n",
    "# It has two main properties:\n",
    "#      - a weigth matrix containing the weights of the layer's neurons. Each line represents a neuron and \n",
    "#        the columns represent its corresponding weights\n",
    "#      - a bias vector, containing the neurons's bias\n",
    "# Since during the backpropagation we need to compute the weights variation using the old ones, the \n",
    "# updated_weights and updated_bias properties store the new values until the update method is called\n",
    "class Layer:\n",
    "    # Create a new Layer with 'size' neurons, each one linked to 'inputs_size' inputs\n",
    "    def __init__(self, size, inputs_size):\n",
    "        self.size = size\n",
    "        self.inputs_size = inputs_size\n",
    "        self.weights = np.array([[random.uniform(-0.1, 0.1) for j in range(inputs_size)] for i in range(size)])\n",
    "        self.bias = np.array([random.uniform(-0.1,0.1) for i in range(size)])\n",
    "        \n",
    "        self.d_weights_current = np.zeros((size, inputs_size))\n",
    "        self.d_bias_current = np.zeros(size)\n",
    "        self.d_weights_old = np.zeros((size, inputs_size))\n",
    "        self.d_bias_old = np.zeros(size)\n",
    "    \n",
    "    # update updates the weights and bias matrices with the values stored in the updated ones\n",
    "    def update(self, eta, alpha):\n",
    "        #self.weights = np.copy(self.updated_weights)\n",
    "        #self.bias = np.copy(self.updated_bias)\n",
    "        \n",
    "        self.weights = self.weights + eta*self.d_weights_current + alpha*self.d_weights_old \n",
    "        self.bias = self.bias + eta*self.d_bias_current + alpha*self.d_bias_old\n",
    "        \n",
    "        self.d_weights_old = self.d_weights_current\n",
    "        self.d_bias_old = self.d_bias_current\n",
    "        \n",
    "    # description prints a layer description\n",
    "    def description(self):\n",
    "        print(\"Layer Info\")\n",
    "        print(\"Weights: \\n\", self.weights)\n",
    "        print(\"Bias: \\n \", self.bias)\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0/(1.0+ math.exp(-x))\n",
    "\n",
    "logistic_vec = np.vectorize(logistic)\n",
    "\n",
    "def logistic_derivate(x):\n",
    "    return x*(1.0-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    # MLP creation. One might pass the MLP layers as parameters or add them later using the add_layer method.\n",
    "    # The classification parameter defines if the MLP will be used for a classification or regression problem\n",
    "    def __init__(self, *layers, classifier=True):\n",
    "        self.classifier = classifier\n",
    "        if classifier:\n",
    "            # Map each class label to a vector with a single 1\n",
    "            # Ex: Class 0 -> [1,0]\n",
    "            #     Class 1 -> [0,1]\n",
    "            self.class_mapping = dict()  \n",
    "            # Unmap each class vector to the corresponding class label\n",
    "            # Ex: [1,0] -> Class 0 \n",
    "            #     [0,1] -> Class 1\n",
    "            self.class_unmapping = dict()\n",
    "            \n",
    "        self.layers = list()\n",
    "        for layer in layers:\n",
    "            self.add_layer(layer)\n",
    "    \n",
    "    # Shortcut to create a classifier MLP\n",
    "    @classmethod\n",
    "    def MLPClassifier(cls, *layers):\n",
    "        return cls(classifier=True, *layers)   \n",
    "    \n",
    "    # Shortcut to create a regressor MLP\n",
    "    @classmethod\n",
    "    def MLPRegressor(cls, *layers):\n",
    "        return cls(classifier=False, *layers)\n",
    "    \n",
    "    # add_layer adds a new layer on the MLP. It verifies whether or not the new layer is compatible with the MLP\n",
    "    def add_layer(self, layer):\n",
    "        # If there's already a layer in the MLP, verify if the new layer is compatible\n",
    "        if len(self.layers) > 0:\n",
    "            if layer.inputs_size != self.layers[-1].size:\n",
    "                print(\"The new layer is incompatible with the MLP\")\n",
    "                print(\"Please, use a layer where each neuron has the same amount of inputs as the number\" \\\n",
    "                     \"of neurons in the MLP last layer\")\n",
    "        \n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    # description prints the info about the MLP layers\n",
    "    def description(self):\n",
    "        print(\"MLP Classifier?: \", self.classifier)\n",
    "        print(\"-------------------------\")\n",
    "        print(\"MLP Info:\")\n",
    "        for layer, i in zip(self.layers, range(len(self.layers))):\n",
    "            print(\"--- Layer: %d ---\" % i)\n",
    "            layer.description()\n",
    "            \n",
    "    # __get_class_mapping gets the class labels in the classes list and builds the mapping dicionaries\n",
    "    # class_mapping and class_unmapping\n",
    "    def __get_class_mapping(self, classes):\n",
    "        class_labels = np.unique(classes)\n",
    "        \n",
    "        for c in range(len(class_labels)):\n",
    "            class_label = class_labels[c]\n",
    "            class_vector = np.zeros(len(class_labels))\n",
    "            class_vector[c] = 1\n",
    "    \n",
    "            self.class_mapping[class_label] = class_vector\n",
    "            \n",
    "            # We can't use a list as a hash key. So transform it into a tuple\n",
    "            self.class_unmapping[tuple(class_vector)] = class_label\n",
    "        \n",
    "    # __convert_class_labels_to_vectors converts a list with class labels to a list with \n",
    "    # vectors that maps each class label\n",
    "    def __convert_class_labels_to_vectors(self, class_labels):\n",
    "        return [self.class_mapping[c] for c in class_labels]\n",
    "    \n",
    "    # __convert_class_vectors_to_labels converts a list with class vectors to a list with \n",
    "    # the corresponding class labels\n",
    "    def __convert_class_vectors_to_labels(self, class_vectors):\n",
    "        return [self.class_unmapping[tuple(class_vector)] for class_vector in class_vectors]\n",
    "        \n",
    "        \n",
    "    # fast_forward computes the ouput for a given input vector\n",
    "    def fast_forward(self,input_v):\n",
    "        # We need to store each layer input in order to perform the backpropagation\n",
    "        self.inputs = list()\n",
    "    \n",
    "        # The input is applied in a layer weights matrix and the bias is added in the result\n",
    "        # Then, the logistic function is applied to each layer neuron result\n",
    "        # For a layer, we have a final output vector where each component i represents the output\n",
    "        # of the neuron i\n",
    "        for layer in self.layers:\n",
    "            self.inputs.append(input_v)\n",
    "            output = logistic_vec(layer.weights @ input_v + layer.bias)\n",
    "            \n",
    "            # The output of the current layer is the input of the next one\n",
    "            input_v = output\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    # train trains the MLP using the examples passed in the samples parameter\n",
    "    # The expected output for each example must be passed in the classes parameter;\n",
    "    # eta represents the MLP learning rate;\n",
    "    # tol represents the error tolerance. The MLP is trained until the cumulative squared error for all example\n",
    "    #     is less than the tol value\n",
    "    # print_status prints the output for each example during the training phase\n",
    "    def train(self, samples, classes, eta=0.5, alpha=0, tol=1e-2, epoch_max=2000, \n",
    "              print_status=False, shuffle=True):\n",
    "        # Map the class labels to output vectors if it's a classification problem\n",
    "        if self.classifier:\n",
    "            self.__get_class_mapping(classes)\n",
    "            classes = self.__convert_class_labels_to_vectors(classes)\n",
    "                \n",
    "        error = tol\n",
    "        new_error = 3*tol\n",
    "        epoch = 0\n",
    "        \n",
    "        # The training stops when the max number of epochs is reached or the Kramer and Sangiovanni-Vicentelly\n",
    "        # criteria is valid. According to it, we can consider that the BP converged when the average mean squared\n",
    "        # error is less than a given tolerance\n",
    "        while (abs(new_error - error) > tol and epoch < epoch_max):\n",
    "            epoch += 1\n",
    "            error = 0\n",
    "            new_error = 0\n",
    "            \n",
    "            # Suffles samples to avoid saturation if training with samples beloging to the same class\n",
    "            # one after another\n",
    "            if shuffle:\n",
    "                samples, classes = shuffle_data(samples, classes)\n",
    "            \n",
    "            for input_v, t in zip(samples, classes):  \n",
    "                # ---- Compute the output for the given input vector ----\n",
    "                output = self.fast_forward(input_v)\n",
    "                \n",
    "                # Compute the mean squared error before the backpropagation\n",
    "                error_sample = pow((np.array(t)-np.array(output)),2)\n",
    "                # We need to sum the error of each component when the output is a vector\n",
    "                error += sum(error_sample)/len(samples)\n",
    "                \n",
    "                if (print_status == True):\n",
    "                    print(\"\\ttraining example: %s from class %s\" % (input_v, t), end = \" \")\n",
    "                    print(\"y = \", output)\n",
    "     \n",
    "                # ---- Backpropagation ----\n",
    "                # Compute the new weights of each layer\n",
    "                # Remark: the udpated weights are stored as a layer property and the layer is updated once \n",
    "                # the backpropagation is finished\n",
    "                # It's necessary to do so in order to compute the delta value for the inner layers. We need \n",
    "                # to use the weights that caused the error to compute the delta instead of the updated weights\n",
    "                for l in reversed(range(len(self.layers))): # Traverse the layers in reversed order\n",
    "                    layer = self.layers[l]\n",
    "             \n",
    "                    deltas = list()\n",
    "                    # Compute the delta for each layer neuron n\n",
    "                    for n in range(len(layer.weights)):\n",
    "                        # Last Layer\n",
    "                        if l == (len(self.layers)-1):\n",
    "                            delta = (t[n]-output[n])*logistic_derivate(output[n])\n",
    "                            \n",
    "                        # Inner Layer\n",
    "                        else:\n",
    "                            # output of the current layer is the input of the next one\n",
    "                            neuron_output = self.inputs[l+1][n]\n",
    "                            # weights of each neuron output\n",
    "                            errors_weights = self.layers[l+1].weights[:,n]\n",
    "                            \n",
    "                            delta = np.dot(delta_next_layer,errors_weights)*logistic_derivate(neuron_output)\n",
    "                              \n",
    "                        # Computes the weights and bias variation for the neuron n\n",
    "                        for w in range(len(layer.weights[n])):\n",
    "                            layer.d_weights_current[n][w] = delta*self.inputs[l][w]\n",
    "                        layer.d_bias_current[n] = delta*1 # bias input = 1\n",
    "                        \n",
    "                        #for w in range(len(layer.weights[n])):\n",
    "                        #    layer.updated_weights[n][w] = layer.weights[n][w] + eta*delta*self.inputs[l][w]\n",
    "                        #layer.updated_bias[n] = layer.bias[n] + (eta*delta*1) # bias input = 1\n",
    "\n",
    "                        # Store the neuron delta\n",
    "                        deltas.append(delta)\n",
    "                    \n",
    "                    # The neurons' delta of the current layer will be used to compute the deltas of the \n",
    "                    # next inner layer\n",
    "                    delta_next_layer = np.array(deltas)\n",
    "                     \n",
    "                # Once the backpropagation is finished for the current example, update all the weigths and bias\n",
    "                for layer in self.layers:\n",
    "                    layer.update(eta, alpha)\n",
    "                    \n",
    "                # Compute the new error mean squared error\n",
    "                output = self.fast_forward(input_v)\n",
    "                error_sample = pow((np.array(t)-np.array(output)),2)\n",
    "                #print(\"error sample: \",error_sample)\n",
    "                new_error += sum(error_sample)/len(samples)\n",
    "            \n",
    "            # End of a epoch\n",
    "            if epoch%1 == 0: # Print status only after each 100 iterations \n",
    "                clear_output(wait=True)\n",
    "                display(\"End of epoch \" + str(epoch) + \". Total Error = \" + str(new_error))\n",
    "        \n",
    "        # End of training         \n",
    "        clear_output(wait=True)\n",
    "        display(\"End of epoch \" + str(epoch) + \". Total Error = \" + str(new_error))\n",
    "        \n",
    "    # predicts gets a list of input samples and returns a list with the predicted outputs\n",
    "    def predict(self, samples):\n",
    "        outputs = list()\n",
    "        for input_v in samples:\n",
    "            probs = self.fast_forward(input_v)\n",
    "            \n",
    "            if self.classifier:\n",
    "                class_pos = np.argmax(probs)\n",
    "                output = np.zeros(len(probs))\n",
    "                output[class_pos] = 1\n",
    "            \n",
    "                #outputs.append(self.class_unmapping[tuple(output)])\n",
    "                outputs.append(output)\n",
    "                \n",
    "            else:\n",
    "                outputs.append(probs)\n",
    "    \n",
    "        if self.classifier:\n",
    "            return self.__convert_class_vectors_to_labels(outputs)\n",
    "        \n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data transforms data in order to all points have mean 0 and variance 1\n",
    "def normalize_data(data):\n",
    "    normalized_columns = list()\n",
    "    for c in range(len(data[0])):\n",
    "        col = data[:,c]\n",
    "        normalized_columns.append((col - np.mean(col))/np.std(col))\n",
    "\n",
    "    return np.array(normalized_columns).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_data transforms data in order to all points be in the interval [0,1]\n",
    "def scale_data(data):\n",
    "    normalized_columns = list()\n",
    "    for c in range(len(data[0])):\n",
    "        col = data[:,c]\n",
    "        normalized_columns.append((col-np.min(col))/(np.max(col)-np.min(col)))\n",
    "\n",
    "    return np.array(normalized_columns).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate(real_classes, predicted_classes):\n",
    "    acc = accuracy_score(real_classes, predicted_classes)\n",
    "    print(\"Accuracy: %.2f%%\" % (acc*100))\n",
    "    \n",
    "    class_labels = np.sort(np.unique(real_classes))\n",
    "    cm = confusion_matrix(real_classes, predicted_classes, labels=class_labels)\n",
    "    df = pd.DataFrame(cm)\n",
    "    df.columns = class_labels\n",
    "    df.index = class_labels\n",
    "    \n",
    "    # Acurracy per class\n",
    "    accs = list()\n",
    "    for c in range(len(cm)):\n",
    "        accs.append(cm[c,c]/sum(cm[c,:]))\n",
    "    df[\"Accuracy\"] = accs\n",
    "        \n",
    "    print(\"Confusion Matrix and Accuracy per class:\")\n",
    "    print(df)\n",
    "        \n",
    "    avg_acc = np.average(accs)\n",
    "    print(\"Average accuracy per class: %.2f%%\" % (avg_acc*100))\n",
    "    return acc, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o problema de classificação, vamos pegar o conjunto de dados fornecidos e vamos normalizá-los de forma que cada valor de feature esteja no intervalo (0,1). Isso será feito para evitar a saturação da saída dos neurônios e melhorar a convergência do algoritmo de aprendizagem.\n",
    "\n",
    "A seguir, tomaremos um conjunto de treinamento consistindo em 70% da base original. Iremos avaliar o impacto da arquitetura da rede assim como da variação de parâmetros de aprendizado no valor de acurácia obtido na classificação do conjunto de teste. \n",
    "\n",
    "Iremos calcular a acurácia total e a acurácia por classe. Uma vez que não estamos impondo penalidades diferentes para erros cometidos em determinadas classes, vamos considerar como melhor arquitetura aquela que fornece maior valor para a acurácia total, embora isso possa não refletir em uma acurácia por classe elevada! Mais tarde, iremos tratar melhor o balanceamento entre as classes a fim de que o aprendizado da rede seja adequado para todas elas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "0           0            7.4              0.70         0.00             1.9   \n",
       "1           1            7.8              0.88         0.00             2.6   \n",
       "2           2            7.8              0.76         0.04             2.3   \n",
       "3           3           11.2              0.28         0.56             1.9   \n",
       "4           4            7.4              0.70         0.00             1.9   \n",
       "\n",
       "   chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
       "0      0.076                 11.0                  34.0   0.9978  3.51   \n",
       "1      0.098                 25.0                  67.0   0.9968  3.20   \n",
       "2      0.092                 15.0                  54.0   0.9970  3.26   \n",
       "3      0.075                 17.0                  60.0   0.9980  3.16   \n",
       "4      0.076                 11.0                  34.0   0.9978  3.51   \n",
       "\n",
       "   sulphates  alcohol category  \n",
       "0       0.56      9.4      Mid  \n",
       "1       0.68      9.8      Mid  \n",
       "2       0.65      9.8      Mid  \n",
       "3       0.58      9.8      Mid  \n",
       "4       0.56      9.4      Mid  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('winequality-red.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.4     0.7     0.      1.9     0.076  11.     34.      0.9978  3.51\n",
      "   0.56    9.4   ]\n",
      " [ 7.8     0.88    0.      2.6     0.098  25.     67.      0.9968  3.2\n",
      "   0.68    9.8   ]]\n",
      "['Mid' 'Mid' 'Mid' ... 'Mid' 'Mid' 'Mid']\n"
     ]
    }
   ],
   "source": [
    "# Separação dos Dados\n",
    "inputs = df[df.columns[1:-1]].values\n",
    "classes = df[df.columns[-1]].values\n",
    "\n",
    "print(inputs[0:2,:])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bad' 'Good' 'Mid']\n",
      "[  63  217 1319]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(classes, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = scale_data(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split - Training - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_inputs, classes, test_size=0.3, \n",
    "                                                    stratify=classes,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different network architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 layer - 5 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.17529597439174865'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training\n",
    "N1 = 5\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(n_classes, N1))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.96%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     3     0   16  0.157895\n",
      "Good    0    21   44  0.323077\n",
      "Mid     8     9  379  0.957071\n",
      "Average accuracy per class: 47.93%\n"
     ]
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Layer - 11 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.1733605344785225'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 11\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(n_classes, N1))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.50%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     0     0   19  0.000000\n",
      "Good    0    13   52  0.200000\n",
      "Mid     2    11  383  0.967172\n",
      "Average accuracy per class: 38.91%\n"
     ]
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Layer - 22 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.15788458253020296'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 22\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(n_classes, N1))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.12%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     0   18  0.052632\n",
      "Good    0    24   41  0.369231\n",
      "Mid     6    16  374  0.944444\n",
      "Average accuracy per class: 45.54%\n"
     ]
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 5 Neurons - 5 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.18066216404897734'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 5\n",
    "N2 = 5\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.33%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     0   18  0.052632\n",
      "Good    0    24   41  0.369231\n",
      "Mid     6    15  375  0.946970\n",
      "Average accuracy per class: 45.63%\n"
     ]
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 5 Neurons - 11 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.17173439984447603'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 5\n",
    "N2 = 11\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.75%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     2     0   17  0.105263\n",
      "Good    0    34   31  0.523077\n",
      "Mid     3    27  366  0.924242\n",
      "Average accuracy per class: 51.75%\n"
     ]
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 5 Neurons - 22 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.16735402370755087'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 5\n",
    "N2 = 22\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.92%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     2     0   17  0.105263\n",
      "Good    0    41   24  0.630769\n",
      "Mid     3    38  355  0.896465\n",
      "Average accuracy per class: 54.42%\n"
     ]
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 11Neurons - 5 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.17190566880333363'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 11\n",
    "N2 = 5\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.92%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     0   18  0.052632\n",
      "Good    0    36   29  0.553846\n",
      "Mid     3    32  361  0.911616\n",
      "Average accuracy per class: 50.60%\n"
     ]
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 11Neurons - 11 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.15694439779030575'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 11\n",
    "N2 = 11\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.58%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     0   18  0.052632\n",
      "Good    0    28   37  0.430769\n",
      "Mid     2    17  377  0.952020\n",
      "Average accuracy per class: 47.85%\n"
     ]
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 11Neurons - 22 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.1755472399373921'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 11\n",
    "N2 = 22\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.96%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     0   18  0.052632\n",
      "Good    0    26   39  0.400000\n",
      "Mid     3    17  376  0.949495\n",
      "Average accuracy per class: 46.74%\n"
     ]
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 22Neurons - 5 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.15953557131697943'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 22\n",
    "N2 = 5\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.29%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     3     0   16  0.157895\n",
      "Good    0    37   28  0.569231\n",
      "Mid     7    34  355  0.896465\n",
      "Average accuracy per class: 54.12%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.541196717512507"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 22Neurons - 11 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.15513180908747362'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 22\n",
    "N2 = 11\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.17%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     0   18  0.052632\n",
      "Good    0    30   35  0.461538\n",
      "Mid     6    17  373  0.941919\n",
      "Average accuracy per class: 48.54%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4853630774683406"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers: 22Neurons - 22 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.15342987352435103'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N1 = 22\n",
    "N2 = 22\n",
    "N = len(X_train[0])\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "random.seed(0)\n",
    "mlp = MLP.MLPClassifier(Layer(N1, N), Layer(N2, N1), Layer(n_classes, N2))\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=400, print_status=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.33%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     2   16  0.052632\n",
      "Good    0    45   20  0.692308\n",
      "Mid     2    40  354  0.893939\n",
      "Average accuracy per class: 54.63%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5462928883981516"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mlp.predict(X_test)\n",
    "evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of epochs used during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos analisar como o número de epochs usadas durante o treinamento interfere na acurácia obtida no conjunto de testes! Para issom, vamos utilizar a arquitetura de rede que obteve melhor desempenho:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = dict()\n",
    "best_N1 = 11\n",
    "best_N2 = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 200. Total Error = 0.1954606515584208'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.12%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     1   17  0.052632\n",
      "Good    0    19   46  0.292308\n",
      "Mid     1    16  379  0.957071\n",
      "Average accuracy per class: 43.40%\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[epochs] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 400 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 400. Total Error = 0.15824168927950283'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.38%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     0   18  0.052632\n",
      "Good    0    34   31  0.523077\n",
      "Mid     7    19  370  0.934343\n",
      "Average accuracy per class: 50.34%\n"
     ]
    }
   ],
   "source": [
    "epochs = 400\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[epochs] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 800 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 800. Total Error = 0.11989796910609325'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.96%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     2     0   17  0.105263\n",
      "Good    0    35   30  0.538462\n",
      "Mid     7    23  366  0.924242\n",
      "Average accuracy per class: 52.27%\n"
     ]
    }
   ],
   "source": [
    "epochs = 800\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[epochs] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 1000. Total Error = 0.11785875370497366'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.50%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     2     2   15  0.105263\n",
      "Good    0    24   41  0.369231\n",
      "Mid     5    21  370  0.934343\n",
      "Average accuracy per class: 46.96%\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[epochs] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 2000. Total Error = 0.09373307280525693'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.71%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     2     0   17  0.105263\n",
      "Good    0    31   34  0.476923\n",
      "Mid     5    27  364  0.919192\n",
      "Average accuracy per class: 50.05%\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=0.5, alpha=0.5, tol=1e-4, epoch_max=epochs, print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[epochs] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{200: (0.83125, 0.4340033261085893), 400: (0.84375, 0.5033506454559086), 800: (0.8395833333333333, 0.5226557068662331), 1000: (0.825, 0.46961245382298017), 2000: (0.8270833333333333, 0.5004593846699109)}\n"
     ]
    }
   ],
   "source": [
    "print(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        200       400       800       1000      2000\n",
      "Total accuracy      0.831250  0.843750  0.839583  0.825000  0.827083\n",
      "Accuracy per class  0.434003  0.503351  0.522656  0.469612  0.500459\n"
     ]
    }
   ],
   "source": [
    "epochs_accs = pd.DataFrame(accs)\n",
    "epochs_accs.index = [\"Total accuracy\", \"Accuracy per class\"]\n",
    "print(epochs_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repare que embora a Loss Function decresça para 1000 e 2000 epochs, ao avaliar o modelo no conjunto de testes, há uma queda na acurácia! isso ilustra a ocorrência de um overfitting do modelo. Portanto, nas próximas etapas, iremos tomar 800 epochs como o máximo de iterações durante a fase de treinamento, uma vez que ela mantém um bom nível de acurácia total, equanto eleva a acurácia por classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epochs = 800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate and Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixando o número de epochs em 500 e utilizando a arquitura de rede com melhor desemenho (2 camadas intermediárias: 5 neurônios na primeira e 11 na segunda), vamos variar os parâmetros learning rate (eta) e momentum (alfa), e vamos observer como eles interferem no aprendizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 0.3 e Momentum = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 800. Total Error = 0.12757128237951523'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.75%\n",
      "Confusion Matrix and Accuracy per class:\n",
      "      Bad  Good  Mid  Accuracy\n",
      "Bad     1     0   18  0.052632\n",
      "Good    0    26   39  0.400000\n",
      "Mid     2    19  375  0.946970\n",
      "Average accuracy per class: 46.65%\n"
     ]
    }
   ],
   "source": [
    "eta = 0.3\n",
    "alpha = 0.3\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 0.3 e Momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 34. Total Error = 0.21697130441251308'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eta = 0.3\n",
    "alpha = 0.5\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 0.3 e Momentum = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.3\n",
    "alpha = 0.8\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 0.5 e Momentum = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.5\n",
    "alpha = 0.3\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 0.5 e Momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.5\n",
    "alpha = 0.5\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 0.5 e Momentum = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.5\n",
    "alpha = 0.8\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 0.8 e Momentum = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.8\n",
    "alpha = 0.3\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 0.8 e Momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.8\n",
    "alpha = 0.5\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 0.8 e Momentum = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.8\n",
    "alpha = 0.8\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate = 1 e Momentum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1\n",
    "alpha = 1\n",
    "\n",
    "random.seed(0)\n",
    "best_layers = [Layer(best_N1, N), Layer(best_N2, best_N1), Layer(n_classes, best_N2)]\n",
    "mlp = MLP.MLPClassifier(*best_layers)\n",
    "mlp.train(X_train, y_train, eta=eta, alpha=alpha, tol=1e-4, epoch_max=best_epochs, \n",
    "          print_status=False, shuffle=True)\n",
    "\n",
    "predicted = mlp.predict(X_test)\n",
    "accs[(eta,alpha)] = evaluate(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in accs.values():\n",
    "    print(k)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-c919d7f68197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mavg_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maccs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtotal_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mavg_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "total_acc = pd.DataFrame()\n",
    "avg_acc = pd.DataFrame()\n",
    "\n",
    "for run, accs in accs:\n",
    "    total_acc.at[str(run[0]), str(run[1])] = accs[0]\n",
    "    avg_acc.at[str(run[0]), str(run[1])] = accs[1]\n",
    "    \n",
    "print(total_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que, quando o learning rate e momentum são de magnitude elevada (próximos à 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos variar o tamanho dos conjuntos de treinamento e teste utilizando a melhor arquitetura encontrada acima e os melhores valores de learning rate e momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Pre Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar nas classificações acima, na base de dados considerada, há poucos exemplos da classe Bad. Como consequência, como estamos dando o mesmo peso para um erro cometido em qualquer classe, a rede acaba por classificar a maioria dos exemplos como sendo pertecentes à classe com maior número de exemplos ('Mid').\n",
    "\n",
    "A seguir, vamos considerar uma base de dados com um número balanceado entre as classes. Para tal, vamos tomar o tamanho da menor classe e escolher exemplos aleatórios das demais classes para igualar esse número.\n",
    "\n",
    "Também vamos considerar uma base de dados com exemplos artificiais que serão criados para igualar o número de exemplos da menor classe com o número de exemplos da classe intermediária. Não igualaremos o número de exemplares pela classe de maior cardinalidade, uma vez que, no mundo real, é normal que haja mais itens de qualidade intermediária que itens de qualidade ruim ou boa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para melhor comparar as base de dados e não ser influenciado pela aleatoriedade com a qual os conjuntos de treinamento e teste são escolhidos, iremos avaliar os modelos utilizando CrossValidation Stratified com 10 folds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
