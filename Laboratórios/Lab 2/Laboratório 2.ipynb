{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratório 2 - Redes Multicamadas\n",
    "\n",
    "- Moisés Botarro Ferraz Silva, 8504135\n",
    "- Thales de Lima Kobosighawa,  9897884\n",
    "- Victor Rozzatti Tornisiello, 9806867"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 1 - MLP e OU-Excluviso\n",
    "\n",
    "Programe uma rede neural MLP, com o algoritmo BP, utilizando a linguagem Python, para resolver o problema do OU-EXCLUSIVO, isto é, encontrando os pesos e thresholds adequados . Use uma função logística como função de ativação e inicialize os pesos aleatórios no intervalo: (-0.1, 0.1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação de um Multilayer Perceptron\n",
    "\n",
    "Primeiramente, vamos implementar um Perceptron multicamadas capaz de receber um número qualquer de camadas e ajustar os seus pesos utilizando Backpropagation com o método do Gradiente (Gradient Descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "\n",
    "Uma camada será representada por uma classe, contendo duas propriedades princiais:\n",
    "    - Uma matriz 'weights', onde cada linha i representa um neurônio da camada e cada coluna j, o peso do neurônio ligado à entrada j\n",
    "    - Um vetor 'bias' onde cada linha i, representa o bias do neurônio i\n",
    " \n",
    "Uma vez que, durante o Backpropagation, precisamos calcular os valores das variações dos pesos utilizando os pesos que geraram o erro, os novos valores serão salvos em duas propriedades: \"updated_weights\" e \"updated_bias\". Ao chamar o método *update* de uma layer, as matrizes de peso e bias serão atualizadas com os novos valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Info\n",
      "Weights: \n",
      " [[ 0.06888437  0.05159088 -0.01588568]\n",
      " [-0.04821665  0.00225494 -0.01901317]]\n",
      "Bias: \n",
      "  [ 0.05675972 -0.03933745]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# Layer represents a MLP Layer\n",
    "# It has two main properties:\n",
    "#      - a weigth matrix containing the weights of the layer's neurons. Each line represents a neuron and \n",
    "#        the columns represent its corresponding weights\n",
    "#      - a bias vector, containing the neurons's bias\n",
    "# Since during the backpropagation we need to compute the weights variation using the old ones, the \n",
    "# updated_weights and updated_bias properties store the new values until the update method is called\n",
    "class Layer:\n",
    "    # Create a new Layer with 'size' neurons, each one linked to 'inputs_size' inputs\n",
    "    def __init__(self, size, inputs_size):\n",
    "        self.size = size\n",
    "        self.inputs_size = inputs_size\n",
    "        self.weights = np.array([[random.uniform(-0.1, 0.1) for j in range(inputs_size)] for i in range(size)])\n",
    "        self.bias = np.array([random.uniform(-0.1,0.1) for i in range(size)])\n",
    "        self.updated_weights = np.copy(self.weights)\n",
    "        self.updated_bias = np.copy(self.bias)\n",
    "    \n",
    "    # update updates the weights and bias matrices with the values stored in the updated ones\n",
    "    def update(self):\n",
    "        self.weights = np.copy(self.updated_weights)\n",
    "        self.bias = np.copy(self.updated_bias)\n",
    "        \n",
    "    # description prints a layer description\n",
    "    def description(self):\n",
    "        print(\"Layer Info\")\n",
    "        print(\"Weights: \\n\", self.weights)\n",
    "        print(\"Bias: \\n \", self.bias)\n",
    "\n",
    "l = Layer(2, 3)\n",
    "l.description()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função de Ativação\n",
    "\n",
    "Nossa rede irá utilizar a função logística como função de ativação. Ela é definida pela funcão **logistic** abaixo. Para poder aplicar essa função em cada elemento de um array numpy, podemos criar uma versão vetorizada utilizando o comando *np.vectorize*\n",
    "\n",
    "Durante o backpropagation, precisamos utilizar a derivada da função logística. Ela é representada pela função **logistic_derivate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 0.5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logistic(x):\n",
    "    return 1.0/(1.0+ math.exp(-x))\n",
    "\n",
    "logistic_vec = np.vectorize(logistic)\n",
    "\n",
    "def logistic_derivate(x):\n",
    "    return x*(1.0-x)\n",
    "\n",
    "logistic_vec(np.array([0, 0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos definir uma classe MLP representando nossa rede.\n",
    "\n",
    "#### Criação\n",
    "\n",
    "Para criar uma MLP, basta usar o construtor **MLP()**. As camadas podem ser passadas para o construtor no momento da instanciação da MLP ou adicionadas depois através do método *add_layer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fast Forward\n",
    "\n",
    "Para o cálculo da saída de uma layer com pesos representados pela matrix W e bias B, a seguinte função é aplicada:\n",
    "\n",
    "$$ output = F(W * I + B) $$\n",
    "\n",
    "Onde I é o vetor de entradas para a camada e F(), a função de ativação. \n",
    "\n",
    "Abrindo a fórmula para uma camada com 2 neurônios, os quais recebem 2 entradas e utilizando a função logística como função de ativação, obtemos:\n",
    "\n",
    "$$  \n",
    "F(\n",
    "\\begin{bmatrix}\n",
    "w_{11}&w_{12} \\\\\n",
    "w_{21}&w_{22}\n",
    "\\end{bmatrix}\n",
    "*\n",
    "\\begin{bmatrix}\n",
    "i_{1} \\\\\n",
    "i_{2} \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_{1} \\\\\n",
    "b_{2} \\\\\n",
    "\\end{bmatrix}\n",
    ") \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{1+exp({w_{11}*i_1 + w_{12}*i_2})} \\\\\n",
    "\\frac{1}{1+exp({w_{21}*i_1 + w_{22}*i_2})} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Ao calcular a saída de uma layer, atribuímos ela à entrada da layer seguinte e aplicamos o processo de forma recursiva até chegarmos ao fim da MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular o novo peso $w_i$ do neurônio $j$ na iteração (k+1), utilizamos a seguinte fórmula\n",
    "\n",
    "$$\n",
    "w_{ij}(k+1) = w_{ij}(k) - \\eta\\frac{\\partial E(w)}{\\partial w_{ij}} \\bigg\\rvert_{w(k)}\n",
    "$$\n",
    "\n",
    "Para o cálculo de $$ \\frac{\\partial E(w)}{\\partial w_{ij}} $$\n",
    "\n",
    "pode-se abrí-lo em\n",
    "\n",
    "$$ \\frac{\\partial E(w)}{\\partial w_{ij}} = \\frac{\\partial E}{\\partial v_j} * \\frac{\\partial v_j }{\\partial w_{ji}} $$\n",
    "\n",
    "Onde $v_j$ é a soma das entradas do neuônio j mais seu bias. \n",
    "\n",
    "Temos que:\n",
    "\n",
    "$$ \\frac{\\partial v_j }{\\partial w_{ji}} = i_{ij} $$\n",
    "\n",
    "one $i_{ij}$ é a entrada i do neurônio j.\n",
    "\n",
    "Entretanto, para o cálculo de $$ \\frac{\\partial E}{\\partial v_j} $$\n",
    "\n",
    "precisamos saber se estamos na última camada ou em uma cada interna. Iremos definir uma nova variável $\\delta_j$ onde \n",
    "\n",
    "$$ \\delta_j = - \\frac{\\partial E}{\\partial v_j} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Última Camada\n",
    "\n",
    "Na última camada, temos acesso direto ao erro cometido pelo MLP, uma vez que podemos comparar diretamente a saída obtida com a saída esperada. Assim,\n",
    "\n",
    "$$ \\frac{\\partial E(w)}{\\partial v_j} = (t_j-y_j)*f' $$\n",
    "\n",
    "onde $t_j$ e $y_j$ são, respectivamente, a saída esperada e a obtida no neurônio j da última camada. $f'$ é a derivada da funçõa de ativação. Para a função logística, temos que\n",
    "\n",
    "$$ f' = y_j(1-y_j) $$\n",
    "\n",
    "Portanto, chegamos à fórmula do novos peso:\n",
    "\n",
    "$$\n",
    "w_{ij}(k+1) = w_{ij}(k) + \\eta \\delta_j i_{ij}\n",
    "$$\n",
    "\n",
    "onde \n",
    "\n",
    "$$ \\delta_j = (t_j-y_j)* y_j*(1-y_j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Camadas Internas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nas camadas internas, não podemos comparar a saída de um neurônio com a saída esperada da rede. Como alternativa, usamos os erros cometidos pelos neurônios da camada à frente conectados à sua saída, ponderados pelos pesos das conexões. Dessa forma, para atualizar o peso do neurônio j\n",
    "\n",
    "$$ \\frac{\\partial E(w)}{\\partial v_j} = - \\sum_k{(\\delta_kw_{kj})}*f' = - <\\delta,w_{j}>*y_j*(1-y_j) $$\n",
    "\n",
    "onde k refere-se ao k neurônio da camada à frente, $\\delta$ é um vetor contendo os valores de $\\delta_k$ para cada neurônio da camada seguinte e $w_j$ refere-se aos pesos da saída do neurônio j.\n",
    "\n",
    "Assim, chegamos à \n",
    "\n",
    "$$ w_{ij}(k+1) = w_{ij}(k) + \\eta \\delta_j i_{ij} $$\n",
    "\n",
    "onde \n",
    "\n",
    "$$ \\delta_j = <k,w_{j}>*y_j*(1-y_j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uso\n",
    "\n",
    "Para utilizar a MLP para fazer predições, basta usar o método predict, passando uma lista de samples para para serem classificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    # MLP creation. One might pass the MLP layers as parameters or add them later using the add_layer method.\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = list()\n",
    "        for layer in layers:\n",
    "            self.add_layer(layer)\n",
    "       \n",
    "    # add_layer adds a new layer on the MLP. It verifies whether or not the new layer is compatible with the MLP\n",
    "    def add_layer(self, layer):\n",
    "        # If there's already a layer in the MLP, verify if the new layer is compatible\n",
    "        if len(self.layers) > 0:\n",
    "            if layer.inputs_size != self.layers[-1].size:\n",
    "                print(\"The new layer is incompatible with the MLP\")\n",
    "                print(\"Please, use a layer where each neuron has the same amount of inputs as the number\" \\\n",
    "                     \"of neurons in the MLP last layer\")\n",
    "        \n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    # description prints the info about the MLP layers\n",
    "    def description(self):\n",
    "        print(\"-------------------------\")\n",
    "        print(\"MLP Info:\")\n",
    "        for layer, i in zip(self.layers, range(len(self.layers))):\n",
    "            print(\"--- Layer: %d ---\" % i)\n",
    "            layer.description()\n",
    "        \n",
    "    # fast_forward computes the ouput for a given input vector\n",
    "    def fast_forward(self,input_v):\n",
    "        # We need to store each layer input in order to perform the backpropagation\n",
    "        self.inputs = list()\n",
    "    \n",
    "        # The input is applied in a layer weights matrix and the bias is added in the result\n",
    "        # Then, the logistic function is applied to each layer neuron result\n",
    "        # For a layer, we have a final output vector where each component i represents the output\n",
    "        # of the neuron i\n",
    "        for layer in self.layers:\n",
    "            self.inputs.append(input_v)\n",
    "            output = logistic_vec(layer.weights @ input_v + layer.bias)\n",
    "            \n",
    "            # The output of the current layer is the input of the next one\n",
    "            input_v = output\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    # train trains the MLP using the examples passed in the samples parameter\n",
    "    # The expected output for each example must be passed in the classes parameter;\n",
    "    # eta represents the MLP learning rate;\n",
    "    # tol represents the error tolerance. The MLP is trained until the cumulative squared error for all example\n",
    "    #     is less than the tol value\n",
    "    # print_status prints the output for each example during the training phase\n",
    "    def train(self, samples, classes, eta=0.5, tol=1e-2, print_status=False):\n",
    "        error = 2*tol\n",
    "        epoch = 0\n",
    "        \n",
    "        while (error > tol):\n",
    "            epoch += 1\n",
    "            error = 0\n",
    "            \n",
    "            for input_v, t in zip(samples, classes):  \n",
    "                # ---- Compute the output for the given input vector ----\n",
    "                output = self.fast_forward(input_v)\n",
    "                \n",
    "                # Compute the squared error\n",
    "                error_sample = pow((np.array(t)-np.array(output)),2)\n",
    "                # We need to sum the error of each component when the output is a vector\n",
    "                error += sum(error_sample)\n",
    "                \n",
    "                if (print_status == True):\n",
    "                    print(\"\\ttraining example: %s from class %s\" % (input_v, t), end = \" \")\n",
    "                    print(\"y = \", output)\n",
    "\n",
    "                    \n",
    "                # ---- Backpropagation ----\n",
    "                # Compute the new weights of each layer\n",
    "                # Remark: the udpated weights are stored as a layer property and the layer is updated once \n",
    "                # the backpropagation is finished\n",
    "                # It's necessary to do so in order to compute the delta value for the inner layers. We need \n",
    "                # to use the weights that caused the error to compute the delta instead of the updated weights\n",
    "                for l in reversed(range(len(self.layers))): # Traverse the layers in reversed order\n",
    "                    layer = self.layers[l]\n",
    "             \n",
    "                    deltas = list()\n",
    "                    # Compute the delta for each layer neuron n\n",
    "                    for n in range(len(layer.weights)):\n",
    "                        # Last Layer\n",
    "                        if l == (len(self.layers)-1):\n",
    "                            delta = (t[n]-output[n])*logistic_derivate(output[n])\n",
    "                            \n",
    "                        # Inner Layer\n",
    "                        else:\n",
    "                            # output of the current layer is the input of the next one\n",
    "                            neuron_output = self.inputs[l+1][n]\n",
    "                            # weights of each neuron output\n",
    "                            errors_weights = self.layers[l+1].weights[:,n]\n",
    "                            \n",
    "                            delta = np.dot(delta_next_layer,errors_weights)*logistic_derivate(neuron_output)\n",
    "                              \n",
    "                        # Computes the new weights and bias for the neuron n\n",
    "                        for w in range(len(layer.weights[n])):\n",
    "                            layer.updated_weights[n][w] = layer.weights[n][w] + eta*delta*self.inputs[l][w]\n",
    "                        layer.updated_bias[n] = layer.bias[n] + (eta*delta*1) # bias input = 1\n",
    "\n",
    "                        # Store the neuron delta\n",
    "                        deltas.append(delta)\n",
    "                    \n",
    "                    # The neurons' delta of the current layer will be used to compute the deltas of the \n",
    "                    # next inner layer\n",
    "                    delta_next_layer = np.array(deltas)\n",
    "                     \n",
    "                # Once the backpropagation is finished for the current example, update all the weigths and bias\n",
    "                for layer in self.layers:\n",
    "                    layer.update()\n",
    "            \n",
    "            # End of a epoch\n",
    "            if epoch%100 == 0: # Print status only after each 100 iterations \n",
    "                clear_output(wait=True)\n",
    "                display(\"End of epoch \" + str(epoch) + \". Total Error = \" + str(error))\n",
    "        \n",
    "        # End of training         \n",
    "        clear_output(wait=True)\n",
    "        display(\"End of epoch \" + str(epoch) + \". Total Error = \" + str(error))\n",
    "        \n",
    "    # predicts gets a list of input samples and returns a list with the predicted outputs\n",
    "    def predict(self, samples):\n",
    "        outputs = list()\n",
    "        for input_v in samples:\n",
    "            outputs.append(self.fast_forward(input_v))\n",
    "    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste da MLP\n",
    "\n",
    "Vamos testar nossa função de criação da MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a valid MLP\n",
      "-------------------------\n",
      "MLP Info:\n",
      "--- Layer: 0 ---\n",
      "Layer Info\n",
      "Weights: \n",
      " [[-0.00468061  0.01667641  0.08162258]\n",
      " [ 0.00093737 -0.04363243  0.05116084]]\n",
      "Bias: \n",
      "  [ 0.0236738  -0.04989873]\n",
      "--- Layer: 1 ---\n",
      "Layer Info\n",
      "Weights: \n",
      " [[0.08194925 0.0965571 ]]\n",
      "Bias: \n",
      "  [0.06204345]\n",
      "-----------------------\n",
      "Creating an invalid MLP\n",
      "The new layer is incompatible with the MLP\n",
      "Please, use a layer where each neuron has the same amount of inputs as the numberof neurons in the MLP last layer\n"
     ]
    }
   ],
   "source": [
    "# MLP creation Test\n",
    "print(\"Creating a valid MLP\")\n",
    "mlp = MLP(Layer(2,3), Layer(1,2))\n",
    "mlp.description()\n",
    "\n",
    "# An invalid MLP\n",
    "print(\"-----------------------\")\n",
    "print(\"Creating an invalid MLP\")\n",
    "mlp = MLP(Layer(2,3), Layer(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora treinar a MLP para aproximá-la de uma porta XOR. Para isso, vamos utilizar o seguinte conjunto de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [[0,0],[0,1],[1,0],[1,1]]\n",
    "classes = [[0],[1],[1],[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar uma MLP com uma cada interna com 2 neurônios e uma cada externa com 1 neurônio apenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 3804. Total Error = 0.009998759091016675'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp = MLP(Layer(2,2), Layer(1,2))\n",
    "mlp.train(samples, classes, eta=0.5, tol=1e-2, print_status=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.04124961]), array([0.95255727]), array([0.95276964]), array([0.06142866])]\n"
     ]
    }
   ],
   "source": [
    "print(mlp.predict([[0,0], [1,0], [0,1], [1,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como observado acima, a rede foi capaz de classificar corretamente os novos examplos passados à ela!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos diminuir e aumentar a taxa de aprendizado para ver seu efeito no treinamento!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 13947. Total Error = 0.009999026964639394'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.03735462]), array([0.95299675]), array([0.953101]), array([0.06456023])]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(Layer(2,2), Layer(1,2))\n",
    "mlp.train(samples, classes, eta=0.3, tol=1e-2, print_status=False)\n",
    "print(mlp.predict([[0,0], [1,0], [0,1], [1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 2273. Total Error = 0.009993149269389968'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.04201139]), array([0.95245161]), array([0.95278139]), array([0.06066028])]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(Layer(2,2), Layer(1,2))\n",
    "mlp.train(samples, classes, eta=0.7, tol=1e-2, print_status=False)\n",
    "print(mlp.predict([[0,0], [1,0], [0,1], [1,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de se esperar, ao diminuir a taxa de aprendizado, precisamos de mais epochs para chegar ao mesmo nível de erro para os exemplos de treinamento. Ao aumentar essa taxa, diminuimos a quantidade de epochs. Entretanto, utilizar uma taxa de aprendizado muito alta pode impedir que o algoritmo chegue ao mínimo global!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante alguns treinamentos, pode ocorrer da rede não conseguir convergir em um tempo baixo. Analisamos o algoritmo cuidadosamente e isso ocorre devido à saturação da saída dos neurônios. Pelo fato da função logística possuir derivada próxima à zero em suas extremidades, ao atingir esse ponto, os pesos começam a ser atualizados muito lentamente.\n",
    "\n",
    "Adicionado a isso, os cálculos computacionais apresentam erros naturais de aproximação que podem piorar esse cenário. Estávamos calculando o valor de delta da seguinte forma, por exemplo, para a camada de saída:\n",
    "\n",
    "(t-output)\\*output\\*(1.0-output)\n",
    "\n",
    "Isso leva a valores diferentes calculando-o através de\n",
    "\n",
    "(t-output)\\(output\\*(1.0-output))\n",
    "\n",
    "Preferimos deixar a versão final do código com a última opção uma vez que o valor da derivada da função logística vai ser calculado primeiramente e depois esse valor vai ser aproximado ao multiplicá-lo com o erro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além disso, reparamos que os valores com os quais os pesos são inicializados têm uma forte influência na convergência do treinamento. Abaixo, por exemplo, escolhemos os valores iniciais para os pesos e bias iguais aos presentes no exemplo do XOR no slide passado em aula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 2050. Total Error = 0.00999990438461254'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.05272843]), array([0.95216913]), array([0.95200198]), array([0.05085128])]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(Layer(2,2), Layer(1,2))\n",
    "mlp.layers[0].weights = np.array([[0.4, 0.5 ], [ 0.8, 0.8]])\n",
    "mlp.layers[0].bias = np.array([ -0.6, -0.2])\n",
    "\n",
    "mlp.layers[1].weights = np.array([[0.4, 0.5 ]])\n",
    "mlp.layers[1].bias = np.array([ -0.6])\n",
    "\n",
    "mlp.train(samples, classes, eta=0.5, tol=1e-2, print_status=False)\n",
    "print(mlp.predict(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso, o treinamento converge sempre após 2050 iterações!\n",
    "\n",
    "Abaixo, vamos alterar a ordem dos exemplos no conjunto de treinamento e verificar como isso afeta o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [[0,0], [1,0], [1,1], [0,1]]\n",
    "classes = [[0],[1],[0],[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 2006. Total Error = 0.009992129286234892'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.05263687]), array([0.95214748]), array([0.05108446]), array([0.95225332])]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(Layer(2,2), Layer(1,2))\n",
    "mlp.layers[0].weights = np.array([[0.4, 0.5 ], [ 0.8, 0.8]])\n",
    "mlp.layers[0].bias = np.array([ -0.6, -0.2])\n",
    "\n",
    "mlp.layers[1].weights = np.array([[0.4, 0.5 ]])\n",
    "mlp.layers[1].bias = np.array([ -0.6])\n",
    "\n",
    "mlp.train(samples, classes, eta=0.5, tol=1e-2, print_status=False)\n",
    "print(mlp.predict(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repare que o aprendizado convergiu com menos iteração que anteriormente. Isso se deve ao fato que, intercalando a classe dos exemplos apresentados à rede, evitamos a saturação dos neurônios, aumentando a velocidade de aprendrizado.\n",
    "\n",
    "A seguir, vamos verificar como a estrutura da rede afeta seu desempenho.\n",
    "Iniciemos com uma rede com 2 neurônios na saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 7398. Total Error = 0.009996677109062057'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.97043488, 0.02965037]), array([0.03317301, 0.96675121]), array([0.03311768, 0.96680663]), array([0.95634348, 0.04373264])]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(Layer(2,2), Layer(2,2))\n",
    "mlp.train([[0,0], [1,0], [1,1], [0,1]], [[1,0],[0,1],[1,0],[0,1]], eta=0.5, tol=1e-2, print_status=False)\n",
    "print(mlp.predict([[0,0], [1,0], [0,1], [1,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar 3 neurônios na camada interna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 3784. Total Error = 0.009999867746803032'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.96878795, 0.03116384]), array([0.03342626, 0.966622  ]), array([0.03345741, 0.96659263]), array([0.95773502, 0.04220717])]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(Layer(3,2), Layer(2,3))\n",
    "mlp.train([[0,0], [1,0], [1,1], [0,1]], [[1,0],[0,1],[1,0],[0,1]], eta=0.5, tol=1e-2, print_status=False)\n",
    "print(mlp.predict([[0,0], [1,0], [0,1], [1,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com 5 neurônios na camada interna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 2978. Total Error = 0.009997331019712289'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.97150779, 0.02848967]), array([0.03216858, 0.96779294]), array([0.03210304, 0.96794808]), array([0.95414193, 0.04584689])]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(Layer(5,2), Layer(2,5))\n",
    "mlp.train([[0,0], [1,0], [1,1], [0,1]], [[1,0],[0,1],[1,0],[0,1]], eta=0.5, tol=1e-2, print_status=False)\n",
    "print(mlp.predict([[0,0], [1,0], [0,1], [1,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para finalizar, utilizemos 10 neurônios na camada intermediária!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 3475. Total Error = 0.009999873956243223'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.96988825, 0.02997582]), array([0.03263652, 0.96773637]), array([0.03295547, 0.96693223]), array([0.95594639, 0.04392384])]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(Layer(10,2), Layer(2,10))\n",
    "mlp.train([[0,0], [1,0], [1,1], [0,1]], [[1,0],[0,1],[1,0],[0,1]], eta=0.5, tol=1e-2, print_status=False)\n",
    "print(mlp.predict([[0,0], [1,0], [0,1], [1,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embora não haja muita variação no número de iterações necessárias para a convergência do treinamento, aumentando o número de neurônios, principalmente na camada interna, a convergência ficou menos sensível à inicialização dos pesos e bias!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solução Final\n",
    "\n",
    "Para o problema do XOR, vamos utilizar uma rede com 5 neurônios na camada interna e 2 neurônios na última camada. Se o valor obtido pelo primeiro neurônio da última camada for maior que o segundo, o exemplo será classificado como 0. Caso contrário, trata-se de um 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 3427. Total Error = 0.009996688438674548'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(0)\n",
    "\n",
    "samples = [[0,0], [1,0], [1,1], [0,1]]\n",
    "classes = [[1,0],[0,1],[1,0],[0,1]]\n",
    "\n",
    "mlp = MLP(Layer(5,2), Layer(2,5))\n",
    "mlp.train([[0,0], [1,0], [1,1], [0,1]], [[1,0],[0,1],[1,0],[0,1]], eta=0.5, tol=1e-2, print_status=False)\n",
    "predicted = mlp.predict(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(x):\n",
    "    if x[0] > x[1]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: [0, 0] \texpected: 0 \tpredicted:0\n",
      "sample: [1, 0] \texpected: 1 \tpredicted:1\n",
      "sample: [1, 1] \texpected: 0 \tpredicted:0\n",
      "sample: [0, 1] \texpected: 1 \tpredicted:1\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(predicted)):\n",
    "    print(\"sample: %s \\texpected: %d \\tpredicted:%d\" % \\\n",
    "          (samples[i], get_result(classes[i]), get_result(predicted[i])))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 2 - Auto-Associador\n",
    "\n",
    "Considere o problema de auto-associador (encoding problem) no qual um conjunto de padrões ortogonais de entrada são mapeados num conjunto de padrões de saída ortogonais através de uma camada oculta com um número pequeno de neurônios.\n",
    "\n",
    "Essencialmente, o problema é aprender uma codificação de padrão com p-bits em um padrão de log2 p-bits, e em seguida aprender a decodificar esta representação num padrão de saída.\n",
    "\n",
    "Pede-se: Construir o mapeamento gerado por uma rede multi-camadas com o algoritmo backpropagation (BP), para o caso do mapeamento identidade, considerando dois casos:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Padrão de entrada e Padrão de Saída: Id(8x8) e Id(8X8), onde Id denota a matriz identidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construção da Rede e Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n",
    "logN = math.ceil(math.log2(N))\n",
    "mlp = MLP(Layer(logN, N), Layer(N,logN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    [1,0,0,0,0,0,0,0],\n",
    "    [0,1,0,0,0,0,0,0],\n",
    "    [0,0,1,0,0,0,0,0],\n",
    "    [0,0,0,1,0,0,0,0],\n",
    "    [0,0,0,0,1,0,0,0],\n",
    "    [0,0,0,0,0,1,0,0],\n",
    "    [0,0,0,0,0,0,1,0],\n",
    "    [0,0,0,0,0,0,0,1]\n",
    "]\n",
    "\n",
    "classes = [\n",
    "    [1,0,0,0,0,0,0,0],\n",
    "    [0,1,0,0,0,0,0,0],\n",
    "    [0,0,1,0,0,0,0,0],\n",
    "    [0,0,0,1,0,0,0,0],\n",
    "    [0,0,0,0,1,0,0,0],\n",
    "    [0,0,0,0,0,1,0,0],\n",
    "    [0,0,0,0,0,0,1,0],\n",
    "    [0,0,0,0,0,0,0,1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 19219. Total Error = 0.0099998663620438'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp.train(samples, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso da Rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos definir uma função para imprimir os exemplos classificados. Caso haja um valor maior ou igual à 0,5 em um neurônio de saída, imprimimos 1. Caso contrário, 0 é impresso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_decoded(decoded):\n",
    "    for d in decoded:\n",
    "        if d >= 0.5:\n",
    "            print(\"1\", end=\" \")\n",
    "        else:\n",
    "            print(\"0\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 0 0 0 0 0 0 \n",
      "0 1 0 0 0 0 0 0 \n",
      "0 0 1 0 0 0 0 0 \n",
      "0 0 0 1 0 0 0 0 \n",
      "0 0 0 0 1 0 0 0 \n",
      "0 0 0 0 0 1 0 0 \n",
      "0 0 0 0 0 0 1 0 \n",
      "0 0 0 0 0 0 0 1 \n"
     ]
    }
   ],
   "source": [
    "for output in predicted:\n",
    "    print_decoded(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como esperávamos, obtivemos na saída a mesma matriz identidade utilizada como entrada!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Padrão de entrada e Padrão de Saída: Id(15x15) e Id(15X15), onde Id denota a matriz identidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construção da Rede e Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15\n",
    "logN = math.ceil(math.log2(N))\n",
    "mlp = MLP(Layer(logN, N), Layer(N,logN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos construir uma matriz identidade com o auxílio do Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.zeros((N,N))\n",
    "for i in range(N):\n",
    "    samples[i][i] = 1\n",
    "samples\n",
    "\n",
    "classes = np.copy(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples:\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples:\")\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: \n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes: \")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'End of epoch 39644. Total Error = 0.009999896097870189'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp.train(samples, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso da Rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mlp.predict(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 \n"
     ]
    }
   ],
   "source": [
    "for output in predicted:\n",
    "    print_decoded(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novamente, obtivemos como saída a mesma matriz identidade usada como entrada! Entretanto, é interessante notar que o número de iterações necessárias para a convergência do treinamento com a entrada 15x15 foi maior que o utilizado para o treinamento da rede menor!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
